[
    {
        "answer": "The attention mechanism is the core component of a Transformer.  \nIt works by turning each input token into three vectors ‚Äì a **query (Q)**, a **key (K)** and a **value (V)** ‚Äì and then comparing every query with all keys to obtain pair‚Äëwise scores. These scores are normalised with a soft‚Äëmax (often after scaling by‚ÄØ‚àöd‚ÄØ) to produce **attention weights**, which are used to compute a weighted sum of the values.  \n\nMathematically, the basic (scaled‚Äëdot‚Äëproduct) attention is expressed in the documents as  \n\n\\[\n\\text{Attention}(Q,K,V)=\\text{softmax}\\!\\left(\\frac{QK^{\\top}}{\\sqrt{d_{k}}}\\right)V .\n\\]\n\nThe resulting weighted sum gives each token a representation that **dynamically attends to all other relevant tokens** in the sequence, capturing contextual relationships and long‚Äërange dependencies without any recurrent or convolutional processing. This self‚Äëattention is applied in each layer of both the encoder and decoder, enabling the model to focus on different parts of the input when generating each output token.",
        "retrieval_context": [
            "mitating the way humans pay attention to the interesting elements around them [118]. Figure 11 describes an example of attention mechanism with respect to the constituents of a sequence, for instance, in this schema, remotely sensed image patches. Vector embedding is performed be- fore calculating attention scores for each patch, followed by normalization using a probabilistic function for gradient stabilization and scaling, then, weighted vector calculation and linear aggregation are implemented to combine the relevant extracted representations.\n\nFigure 11: Attention mechanisms\n\n.\n\nMany creative models are based on these mechanisms, for example:\n\nTransformers: introduced by Vaswani et al. [119], transformers are architechtures used mainly in Natural Language Processing (NLP), but adapted to multiple applications such as computer vision, via Vision transformers (ViTs) [120].\n\nAs indicated in Figure 12, a transformer consists of an encoder processing the inputs and a decoder generating the output. It is noted that each of these two components is made up of a single layer of feed forward neural networks, as well as residual addition and normalization to preserve the information transmitted by the model, along with positional encoding to manage sequence order. Each encoding-decoding layer is reinforced by a self-attention mechanism in order to extract dependencies between sequence components, including image patches, tokens, etc. In the attention mechanism in question, three vec",
            " |                         | ‚úì                       |                         | ‚úì                       |                         |\n| Instruction Baseline    |                         | ‚úì                       | ‚úì                       |                         |                         |                         | ‚úì                       | ‚úì                       |                         |                         |                         |                         | ‚úì                       | ‚úì                       |                         |\n\ndevelopment of robust and reliable steering mechanisms, making it difficult to determine the best approach for ensuring model safety and controllability across diverse scenarios.\n\nThis paper argues that the effectiveness of steering methods is closely tied to the task itself, and surprisingly, simple prompt-based methods can be remarkably effective. We propose a key insight: many of the failures of prompt-based steering can be overcome with a targeted adjustment to the model's internal processing . Specifically, we find that manipulating the attention mechanism - a core component of modern transformer architectures - provides a powerful yet simple lever for ensuring steering instructions are consistently followed. This approach is not merely empirical; it is motivated by prior theoretical work that argues that in-context rule following in transformer-based models can be controlled by manipulating attention on instructions [8].\n\nBuildi",
            "tion mechanism enables dynamic weighting of input tokens, capturing long-range sequential dependencies without recurrence or convolution. The vanilla Transformer consists of an encoder-decoder structure, with each layer comprising multi-head self-attention and position-wise feed-forward networks. This design enables efficient computation and scalability, making it ideal for large-scale language modeling tasks. Key components include:\n\n- Multi-head self-attention. Central to the Transformer is the self-attention mechanism, which allows the model to weigh the importance of different parts of the input sequence when encoding each token. This mechanism computes attention scores in a pairwise way by comparing query, key, and value vectors derived from the input embeddings. The resulting weighted sum captures contextual relationships, enabling the model to understand the significance of each token in relation to others. Multi-head attention extends the\n\nself-attention mechanism by employing multiple attention heads, each learning different aspects of the input representation. The outputs of these heads are concatenated and linearly transformed, allowing the model to capture a diverse range of semantic features and relationships within the data. Formally, the input sequence X ‚àà R n √ó d model is processed through h parallel attention heads, where each head i computes scaled dot-product attention as:\n\n$$Q _ { i } & = X W _ { i } ^ { Q } , \\quad K _ { i } = X W _ { i } ^ { K } , \\quad ",
            " This growing context window causes attention dilution in the transformer attention mechanism. At each position in the sequence, the model computes how much to 'attend to' every other position when processing information. The attention weight determines what fraction of each context position contributes to the current computation. For a query position i attending to key position j in a sequence of length N , where q i and k j are learned representations and d is the embedding dimension, the attention weight is computed as:\n\n$$\\alpha _ { i j } = \\frac { \\exp ( q _ { i } ^ { \\top } k _ { j } / \\sqrt { d } ) } { \\sum _ { \\ell = 1 } ^ { N } \\exp ( q _ { i } ^ { \\top } k _ { \\ell } / \\sqrt { d } ) } .$$\n\nAs N grows, the normalizing denominator accumulates more terms, forcing each individual attention weight Œ± ij to become smaller since all weights must sum to 1. This spreads attention across more positions, diluting focus on relevant information [45, 46]. Additionally, intermediate outputs such as computed statistics may contain errors that propagate if not detected early.\n\nThe thinking tool prompts agents to explicitly articulate their current observation, planned action, and reasoning before proceeding, forcing distillation of relevant context. The quality check tool\n\ncomplements this by applying domain-specific validation rules to intermediate outputs, flagging zerovalued aggregations suggesting data filtering errors and checking for statistical plausibility. Explicit thinking ",
            "he flow of data through the network, from input to output. Here is a detailed description of the components and their arrangement:\n\n1. **Encoder**:\n   - The encoder is on the left side of the diagram.\n   - It consists of multiple layers, each containing a combination of self-attention and feed-forward mechanisms.\n   - The self-attention mechanism is represented by a yellow box, and the feed-forward mechanism is represented by a blue box.\n   - Each layer is connected by arrows indicating the flow of data.\n   - The encoder is connected to the decoder via a series of connections.\n\n2. **Decoder**:\n   - The decoder is on the right side of the diagram.\n   - It also consists of multiple layers, each containing self-attention and feed-forward mechanisms.\n   - The self-attention mechanism\n\nSelf Attention After mapping the coordinates to d model via Projection and adding positional information through Positional Encoding, the x-coordinate sequences are encoded by the Encoder's Self-Attention , allowing sequences to interact and capture the bidirectional information of them. An attention mask marks valid sequences and padding.\n\nThe input, represented as H ‚àà R T √ó d model , is linearly transformed into queries ( Q ), keys ( K ), and values ( V ) using weight matrices W Q , W K , W V ‚àà R d model √ó d model . Each head computes attention as:\n\n$$O _ { i } = \\text {softmax} \\left ( \\frac { Q _ { i } K _ { i } ^ { T } } { \\sqrt { d _ { h e a d } } } + M \\right ) V _ { i }$$\n\nWhere M is the att",
            "or [‚àÜ x, ‚àÜ y, s ] , where ‚àÜ x and ‚àÜ y are the offsets along the x - and y -axes, and s ‚àà { 0 , 1 } denotes the pen state (0 = pen-down, 1 = pen-up). After the linear projection and positional encoding, we obtain Z = [ z 1 , z 2 . . . , z L ] , where z i ‚àà R d and L is the number of stroke points.\n\nGiven the stroke embedding sequence Z , the Transformer decoder first applies masked selfattention to enforce autoregressive dependencies among stroke points. It then performs crossattention with the text representations C to align each generated stroke with the corresponding text content. To ensure that each decoding step attends to the most relevant region of the input text, we introduce a Gaussian-shaped cross-attention mask. For each decoder time step t ‚àà [1 , L ] , we define\n\nFigure 1: The proposed TrInk framework (left) and Gaussian memory mask (right).\n\nThe image consists of a diagram that appears to be a schematic representation of a neural network architecture, specifically a variant of a transformer model. The diagram is divided into two main sections: the left side shows the architecture of the model, and the right side presents a heatmap visualization of the model's performance.\n\n### Left Side: Architecture Diagram\n\n1. **Transformer Encoder (E)**:\n   - The encoder is composed of multiple layers of a transformer architecture.\n   - Each layer includes a linear layer (f(i)) and a self-attention mechanism.\n   - The input to the encoder is a sequence of points, which is not e",
            "ation for improved architectural design and training strategies in both synthetic and natural language domains.\n\n## B More training details\n\nAdditional training information: We use the Adam optimizer Diederik [2014] , and a learning rate of 10 -4 for all models.\n\nComputational resources: We used Nvidia A-100 GPUs to train the different versions of transformer models from scratch.\n\n## C Proofs\n\n## C.1 Some Basics\n\nIn this section, we unpack an attention only transformer model to provide mathematical expression of ÀÜ f Œ∏ . This will enable us to formally show certain properties of ICL. A transformer is a neural network model that maps a sequence of input vectors ( x 1 , ¬∑ ¬∑ ¬∑ , x n ) to a corresponding sequence of output vectors, through a stack of layers. Each layer in the transformer operates on a sequence of vectors X ( l ) = ( x ( l ) 1 , x ( l ) 2 , ..., x ( l ) n ) , which represents the sequence at layer l , and produces a new matrix X ( l +1) for the next layer.\n\nWe focus on the case of autoregressive, decoder-only transformer model composed of L layers and H attention heads. In each layer, the input sequence is first processed by a multi-head self-attention mechanism. Each attention head computes attention weights and context vectors independently. The attention head operation is defined as:\n\n$$( x _ { 1 } ^ { ( l ) } , \\dots , x _ { n } ^ { ( l ) } ) \\rightarrow ( A ^ { h , ( l + 1 ) } ( x _ { 1 } ^ { ( l ) } ) , \\dots , A ^ { h , ( l + 1 ) } ( x _ { n } ^ { ( l ) } ) ",
            "diagram of the q - q model for q - q neural networks\n\nand feeding ÀÜ x i +1 back into the model. Variations include greedy sampling (selecting the most probable token) and topn sampling 1 [11].\n\n## 2.1.4 Transformer Architecture\n\nThe Transformer [30] has revolutionized natural language processing. It follows an encoder-decoder structure, replacing recurrent layers with multi-head self-attention layers and position-wise feed-forward networks. Both encoder and decoder comprise stacks of identical layers, enabling parallel processing and capturing long-range dependencies more effectively than traditional RNNs.\n\nAttention mechanism. The attention mechanism is the core component of the Transformer architecture. It allows the model to focus on different parts of the input sequence when producing each element of the output sequence.\n\nThe basic attention function is called 'Scaled Dot-Product Attention'. It operates on queries (Q), keys (K), and values (V). The attention output is computed as:\n\n$$A t t e n t i o n ( Q , K , V ) = \\text {softmax} \\left ( \\frac { Q K ^ { T } } { \\sqrt { d _ { k } } } \\right ) V$$\n\nwhere d k is the dimension of the keys. The Figure 2.1 illustrates this computation. The scaling factor 1 ‚àö d k is introduced to counteract the effect of the dot products growing large in magnitude for large values of d k .\n\nInstead of performing a single attention function, the Transformer employs multihead attention, which allows the model to jointly attend to information fr",
            " a format that can be processed by the neural network.\n\n3. **Attention Mechanism**:\n   - The attention mechanism is a central component of the Transformer model. It is represented by a large, red, rectangular box with the label \"Attention\". The attention mechanism allows the model to focus on different parts of\n\nDropout layer for further learning, ultimately achieving accurate predictions of the time series data.\n\nTo evaluate the performance of the LSTM-Transformer model, the following metric is employed:\n\nMean Absolute Error (MAE): MAE measures the average absolute deviation between predicted values and actual values, providing a clear indication of the real errors in predictions.\n\n$$M A E = \\frac { 1 } { n } \\sum _ { t = 1 } ^ { n } | \\hat { y } _ { t } - y _ { t } |$$\n\nMean Squared Error (MSE): MSE is defined as the average of the squared differences between the predicted values and the actual values. It is a useful metric for assessing the variability of a model's predictions and provides insight into how well the model captures the underlying trend of the data. MSE penalizes larger errors more severely than smaller ones, which can be particularly useful when outliers are present in the dataset.\n\n$$M S E = \\frac { 1 } { n } \\sum _ { t = 1 } ^ { n } \\left ( \\hat { y } _ { t } - y _ { t } \\right ) ^ { 2 }$$\n\nwhere GLYPH(cmap:d835) GLYPH(cmap:d835) represents the predicted values, GLYPH(cmap:d835) GLYPH(cmap:d835) represents the actual values, and GLYPH(cmap:d835) is the num"
        ]
    },
    {
        "answer": "Graph neural networks use a **message‚Äëpassing** scheme.  \nAt each layer a node collects the feature vectors of its neighbors, applies a **permutation‚Äëinvariant aggregation function** (e.g., sum, mean, max, or an attention‚Äëbased weighting) to combine those neighbor messages, and then feeds the aggregated result together with its own current representation into a **learnable update function** that produces the node‚Äôs new embedding.  \n\nThis process is expressed in the documents as  \n\n\\[\nh_i^{(l+1)} = \\text{UPDATE}^{(l)}\\Big( h_i^{(l)},\\; \\text{AGG}^{(l)}\\big(\\{h_j^{(l)} \\mid v_j \\in \\mathcal N(v_i)\\}\\big) \\Big),\n\\]\n\nwhere‚ÄØ\\(\\text{AGG}^{(l)}\\) denotes the neighbor‚Äëaggregation step and‚ÄØ\\(\\text{UPDATE}^{(l)}\\) the subsequent transformation. The iterative application of this step lets each node incorporate information from its local neighborhood (and, over multiple layers, from farther‚Äëaway nodes).„ÄêDocument excerpts: ‚ÄúGraph Neural Networks (GNNs) are models designed to process graph data by iteratively aggregating and transforming information from a node's neighbors‚Ä¶ This message‚Äëpassing mechanism updates the feature representation of each node‚Äù and the formula above.„Äë",
        "retrieval_context": [
            "ch step, nodes exchange information (messages) with their neighbors. This process allows nodes to aggregate information from their local neighborhood, enabling them to make informed decisions about their own properties or the properties of neighboring nodes [144]. One of the most prominent features of GNNs, is their ability to incorporate contextual information from neighboring nodes [158]. For example, in a node classification task, each node can aggregate information from its neighboring nodes to refine its own-and possibly not so far known-classification label.\n\n## 2.1 GNN variations\n\nWhile GNNs offer powerful capabilities for learning from graph-structured data, researchers have developed a number of variants to address specific challenges that exist in the different application domains. Some notable variants of GNNs include GCNs [66], GATs [134] and GraphSAGE [50].\n\nGCNs generalize the concept of convolutional neural networks to graphs, enabling effective information propagation and feature learning across graph nodes. They have been successfully applied in tasks such as node classification and graph classification [66]. At the heart of GCNs lies the notion of graph convolution, which enables nodes to aggregate and propagate information from their neighbors. Mathematically, the graph convolution operation in GCN can be expressed as follows:\n\n$$H ^ { ( l + 1 ) } = \\sigma ( \\widetilde { D } ^ { - \\frac { 1 } { 2 } } \\widetilde { A } \\widetilde { D } ^ { - \\frac { 1 } { 2 }",
            "aggregator functions. The intuition is that at each training iteration these functions learn to combine information from neighboring nodes at different depths, as shown in Figure 3.\n\nFig. 3: GraphSAGE architecture [50]\n\nThe image is a diagram that illustrates a process involving graph analysis and label prediction. It consists of three main sections, each depicting a different stage of the process.\n\n### Description of the Image:\n\n1. **Sample Neighborhood (Leftmost Section):**\n   - This section shows a circular diagram with a central node and several surrounding nodes.\n   - The central node is highlighted in red, indicating it is the main focus.\n   - The surrounding nodes are colored in shades of red, green, and blue.\n   - Arrows connect the central node to the surrounding nodes, suggesting a relationship or interaction between them.\n   - The diagram is labeled with \"1. Sample neighborhood,\" indicating that this is the initial stage of the process.\n\n2. **Aggregate Feature Information from Neighbors (Middle Section):**\n   - This section shows a more complex diagram with multiple nodes and connections.\n   - The central node is again highlighted in red.\n   - Surrounding\n\nThe equation encapsulating the process in Figure 3 can be represented as:\n\n$$h _ { v } ^ { ( k ) } = A G G R E G A T E _ { k } \\left ( \\{ h _ { u } ^ { ( k - 1 ) } , \\forall u \\in N ( v ) \\} \\right )$$\n\nThe aggregated representation of node v at layer k is represented by the term h ( k ) v and captures the inform",
            "ere ùëë is the feature dimension. In the context of domain adaptation, we distinguish between a source graph ùê∫ ùë† and a target graph ùê∫ ùë° . They share the same label space but have different data distributions. The source graph ùê∫ ùë† is accompanied by a label matrix ùëå ‚àà { 0 , 1 } ùëõ √ó ùê∂ , where ùê∂ is the number of classes. We assume a Graph Neural Network (GNN), denoted as ùëì ùë† ùê∫ùëÅùëÅ , has been pre-trained on the source graph ùê∫ ùë† for a node classification task. The objective of Test-Time Graph Domain\n\nAdaptation (TT-GDA) is to effectively improve the performance of the pre-trained model ùëì ùë† ùê∫ùëÅùëÅ on the target graph ùê∫ ùë° . A critical constraint is that the source graph ùê∫ ùë† is not accessible during the adaptation process, typically due to privacy concerns.\n\n## 3.2 Graph Neural Networks\n\nGraph Neural Networks ( GNNs ) are models designed to process graph data by iteratively aggregating and transforming information from a node's neighbors [13, 38, 38, 43]. This message-passing mechanism updates the feature representation of each node, enabling GNNs to learn powerful node embeddings. A typical GNN layer can be expressed as:\n\n$$h _ { i } ^ { ( l + 1 ) } = \\text {UPDATE} ^ { ( l ) } \\left ( h _ { i } ^ { ( l ) } , A G G ^ { ( l ) } \\left ( \\{ h _ { j } ^ { ( l ) } | v _ { j } \\in \\mathcal { N } ( v _ { i } ) \\} \\right ) \\right ) \\quad ( 1 )$$\n\nwhere ‚Ñé ( ùëô ) ùëñ is the feature vector of node ùë£ ùëñ at layer ùëô , and N( ùë£ ùëñ ) is the set of neighbors of node ùë£ ùëñ . AGG ( ùëô ) denote the aggragation functio",
            "Our contributions can be summarized as follows:\n\n- We propose a structural graph augmentation, generating informative edges connecting non-adjacent nodes by exploring their correlation with high structural similarity. The generating edges between distant nodes could then assist the model in mitigating degree bias, as low-degree nodes can receive more messages from correlated nodes while messages for high-degree nodes will be reduced.\n- We propose structural self-attention to learn the relations between correlated nodes based on their structural similarity. The structural self-attention could capture the high-order proximity and the node roles, leading to the model's ability to learn global graph structures.\n- We propose the SSL task to preserve graph structure and regularize the graph augmentation. The SSL task can allow our model to reconstruct the global graph structure without using label information while using the augmentation module to mitigate degree bias.\n\n## 2 RELATED WORK\n\nAs degree bias originates from imbalanced distributions of the number of neighbors, several studies have proposed augmentation schemes to modify the neighborhoods [7], [21], [22], [23]. Early augmentation methods, e.g., GRACE [24] and RGRL [8], randomly modify input graphs and then employ GNNs (Graph Neural Networks) with contrastive objectives to learn representations, not considering both proximity and structural similarity. GCA [15] improves GRACE by using node centrality-based augmentation tha",
            "e [32] as the criterion to query dominant information in traffic flows. Local-sensitive hashing (LSH) was introduced by Reformer [33] to approximate attention by allocating similar queries. However, although plenty of Transformer-based methods [19, 34, 35] have shown superior performance compared to CNN or RNN-based models, they normally suffer from quadratic memory and runtime overhead.\n\n## 2.2 Graph-based model\n\nRecently, many studies have attempted to use graph neural networks to model the correlation of spatial-temporal sequences in TFP. Graph convolutional network (GCN) [11] enables extracting high-level features of target road nodes by aggregating information from neighbor nodes. Specifically, categorized by the graph type, there are two types of methods in general. Spectral-type GCN [36] extended graph conventional convolution operation by Laplacian spectrum in the spectral domain. However, it suffers from expensive computational costs due to the calculation of all the eigenvalues of the Laplacian matrix. Thus, ChebNet [37] was proposed to approximate the graph convolution by the Chebyshev-polynomial expansion of the eigenvalue diagonal matrix, and RGSL [15] was proposed to model explicit spatial relation graph and extract implicit temporal graph simultaneously by a Laplacian matrix mixed-up module. Concentrating on TFP, DSTAGNN [18] optimized the multi-head attention mechanism to capture dynamic spatial relevance and replace pre-defined static spatial graphs with lear",
            "nformation and proximity between non-adjacent nodes hinders GNNs from preserving global graph structures.\n\nIn this study, we propose a novel framework, DegFairGT, to mitigate degree biases via structural graph augmentations and graph transformers. The main idea is to exploit non-adjacent nodes with high structural similarity to form informative edges rather than random edge perturbation through graph augmentation, which could generate arbitrary edges and provide noisy information as well as disrupt the original graph structure. More precisely, we first\n\nFig. 2: Structural graph augmentation adds intracommunity edges between low-degree nodes and removes edges between nodes with different degrees or communities. Given an input graph (left), our structural graph augmentation adds intra-community edges between lowdegree nodes and removes edges between nodes with different degrees or communities. Our structural augmentation can enable each node to obtain more valuable messages from neighbors within the community and k-hop distance through message passing (right).\n\nThe image depicts a schematic representation of a process involving graph theory and community detection. The diagram is divided into two main sections, each illustrating a different stage of the process.\n\n### Left Section:\n1. **Graph Representation**:\n   - The left side of the image shows a graph with nodes (represented by circles) and edges (represented by lines connecting the nodes).\n   - The nodes are colored differe",
            "e layers of nodes (neurons) connected by weights and is primarily used for supervised learning tasks, such as classification.\n\n(2) Graph Neural Networks: GCN (Kipf and Welling, 2016) is a fundamental method based on convolutional neural networks which operates directly on graph-structured data. GAT (VeliÀá ckovi¬¥ c et al., 2017) computes the hidden representations of each node in the graph by first learning the importance of its neighbors and then aggregating information from them. GIN (Xu et al., 2019) develop a simple architecture that is provably the most expressive among the class of GNNs and is as powerful as the Weisfeiler-Lehman graph isomorphism test. GraphSAGE (Hamilton et al., 2017a) present a general inductive framework that leverages node feature information (e.g., text attributes) to efficiently generate node embeddings for previously unseen data. Graphormer (Ying et al., 2021) is built upon the standard Transformer (Vaswani, 2017) architecture, and could attain excellent results on a broad range of graph representation learning tasks. Graphormer propose several simple yet effective structural encoding methods to help Graphormer better model graph-structured data.\n\n(3) Ensemble Models: Bagging (Breiman, 1996) is an ensemble method that improves model stability and accuracy by training multiple base learners on bootstrap samples and aggregating their predictions through voting or averaging. Stacking (Wolpert, 1992) is an ensemble technique that combines predictions",
            " and dependencies among nodes [9], [10].\n\nSome observation range expansion methods aim to expand the range of neighborhoods surrounding target nodes to capture high-order relationships and more information at each node representation update [11], [12], [13]. Several studies aim to adjust the model weights to distill sufficient messages during the message-passing aggregation [14]. Nevertheless, as the degree bias originates from graph structure imbalance, aggregation modulation can not adapt properly to the varying size and structure graphs.\n\nAlthough graph neural network strategies have shown effectiveness in mitigating degree bias, there are two limitations in existing studies that still limit their ability. First, it is challenging to discover correlated nodes with high structural similarity to provide informative messages for each target node. Most graph augmentation-based methods ignore the structural similarity between node pairs when forming edges, forcing GNNs to learn noisy and redundant messages. The augmentation methods randomly add or drop edges to generate multiple graph views, which could mainly benefit low-degree nodes to obtain more messages from distant nodes [8], [15], [16]. While randomly adding edges can bring more messages from non-adjacent nodes, the nodes could contain noisy and irrelevant information, which could force GNNs to aggregate redundant messages, resulting in suboptimal performance. That is, pairs of nodes formed into edges should be strongly ",
            "DTP achieves the state-of-the-art performance.\n- GAMDTP merges a score mechanism to evaluate the prediction results of proposal and refinement to improve the performence of the refine process.\n\n## 2 Related work\n\n## 2.1 GNNs and Temporal Models for Trajectory Prediction\n\nThe development of accurate and efficient trajectory prediction models is critical for autonomous driving, as they allow for anticipating the future states of traffic agents ensuring safety and operational stability for real-time decisions. To model the social spatial and temporal interactions between agents and agents, agents and lanes, [4,5] apply message-passing GNNs and encode agents and lanes as nodes, speed, direction and other dynamic information as edges. GNNs work by iteratively gathering information from neighboring nodes to update the current node's representation, with different GNN types employing distinct aggregation and update functions. This process enables GNNs to learn representations that encapsulate the graph data's topological structure. To model history trajectory and other sequence data, early approaches relied heavily on Recurrent Neural Networks(RNNs)[32] and Long Short-Term Memory networks(LSTMs)[10] to\n\nFig. 2. Our proposed Graph Attention Mamba module, which integrates Mamaba block and graph attention block. The input features include node features and edge features, which first normalized through a Layernorm(LN) layer before processed by Mamba and GAT blocks. The output from these"
        ]
    },
    {
        "answer": "Supervised learning and unsupervised learning differ mainly in the type of data they use and the goals they pursue:\n\n* **Supervised learning** trains a model on **labeled data**‚Äîeach training example is paired with a known output (the ‚Äúground‚Äëtruth‚Äù label). The model learns the explicit mapping from inputs to these outputs, which allows it to make predictions on new, unseen data. Typical supervised tasks are **classification** (assigning a category) and **regression** (predicting a continuous value)„Äê...„Äë.\n\n* **Unsupervised learning** works with **unlabeled data**. Since there are no predefined target labels, the model‚Äôs aim is to uncover hidden structure in the data, such as **clusters**, **latent factors**, or lower‚Äëdimensional representations. It is used for tasks like data grouping, dimensionality reduction, and discovering relationships among variables„Äê...„Äë.",
        "retrieval_context": [
            "ine a policy or an optimal path) or to solve specific tasks (e.g: object detection and classification). Based on the way algorithms learn from data, it can be divided into three learning paradigms: supervised learning , unsupervised learning , and reinforcement learning . The primary differences among those three are in the types of result being produced, and the way neural network models validate and calibrate those results. Figure 1 illustrates such differences of those learning paradigms.\n\n- Supervised learning involves training a model from labeled data. In other words, a neural network learns to predict the future output based on what happened in the past input. It will identify any patterns or trends (also known as weight ) that are historically correlated with the ground-truth outcomes and then use them to perform future prediction.\n- Unsupervised learning analyzes and clusters unlabeled data sets; aiming to discover hidden patterns in data without the need for human intervention. This type of learning is mostly used for grouping data in a meaningful way, representing hight-dimensional data set in a low-dimensional form, and associating relationships between variables in a large dataset.\n- Reinforcement learning , in the other hand, does not rely on any pre-learned pattern. Instead, it mimics the trial-and-error learning process that humans use to achieve its goals. Specifically, neural network models in reinforcement learning use a reward-and-punishment paradigm as th",
            ". Instead, it mimics the trial-and-error learning process that humans use to achieve its goals. Specifically, neural network models in reinforcement learning use a reward-and-punishment paradigm as they processes data. The algorithm constantly tries new things to learn and improving upon current approaches to maximize the defined reward.\n\nFig. 1. Three paradigms in deep learning: supervised, unsupervised, and reinforcement. (a) In supervised learning, models are trained on labeled data to mostly perform the task of classification (where results are categories) and regression (where results are continuous values); (b) in unsupervised learning, models are trained on unlabeled data to identify associated relationships in a dataset, insightful cluster data group, and reduce high dimensional data form.\n\nThe image appears to be a schematic diagram that outlines a process involving machine learning and decision-making. The diagram is divided into three main sections, each representing a different stage of the process.\n\n### Section (a):\n- **Title**: This section is labeled \"Labeled Data\" and shows a flow of data from left to right.\n- **Data**: The data is represented by a series of geometric shapes (circles, triangles, and rectangles) arranged in a specific pattern.\n- **Machine**: The machine is depicted as a series of interconnected gears, indicating a complex system.\n- **ML Model**: The machine is connected to a model, which is represented by a box with a gear symbol.\n- **Predictio",
            "of decision-making trial and error', is a method for optimizing performance in a particular setting by implementing decisions. The results of this type of machine learning can be used in various contexts, such as de novo drug design for decision-making or experimental design for implementation. Modeling and quantum chemistry help achieve these goals [22] (Figure 2).\n\nFigure 2: Machine learning techniques in drug discovery and development\n\nThe image is a diagram that illustrates the three main types of machine learning: Supervised Learning, Unsupervised Learning, and Reinforcement Learning. The diagram is circular, with each type of learning represented by a colored section and connected to the central concept of \"Machine Learning.\"\n\n1. **Supervised Learning**:\n   - This section is colored blue.\n   - It is located at the bottom left of the diagram.\n   - The text \"Supervised Learning\" is written in black.\n   - Supervised learning involves training a model on a labeled dataset, where the correct output is known. The model learns to map inputs to outputs based on this labeled data.\n\n2. **Unsupervised Learning**:\n   - This section is colored green.\n   - It is located at the top left of the diagram.\n   - The text \"Unsupervised Learning\" is written in black.\n   - Unsupervised learning involves training a model on an unlabeled dataset, where the correct output\n\nThe  artificial  neural  networks  used  in  deep  learning  (DL)  can  learn  and  adapt  from  massive experimental data. ",
            "p learning uses the back-propagation technique to adjust the internal parameters of the model in order to recalculate these representations at each epoch.\n\nIn this section, a comprehensive overview of all commonly used AI methods and models is given, along with the main concepts put forward in the context in question.\n\n'\n\n## 3.1 Machine Learning (ML) models\n\nIn order to carry out the various tasks commonly performed by AI, such as regression where the values of scalars are predicted, classification where a label is assigned to a specific piece of data, or clustering where a partitioning of data is proposed, a multitude of machine learning models are presented in the literature. In this subsection, different models are explored depending on the nature of the data used. In fact, two types of learning are distinguished: supervised learning, when the training data are labeled, and unsupervised learning, in the opposite case [42].\n\n## 3.1.1 Supervised learning\n\nIn this context, the data used for training is labeled, i.e. each data record is associated with a specific value. This value is then employed during the learning phases as a reference value, used to represent the links between inputs and outputs [43]. In this subsection, the most notable regression and supervised classification models are explored.\n\nLinear Regression: is an approach for modelling the relationship between a scalar dependent variable and explanatory variables, the case of a single explanatory variable being ",
            "of supervised learning, self-supervised learning, and pair-efficient self-supervised learning.\n\nThe image is a diagram that illustrates a comparison between two types of learning methods: supervised learning and self-supervised learning. The diagram is divided into three main sections, each representing a different learning approach.\n\n### Top Section:\n- **Frozen**: This section is labeled \"Frozen\" and is depicted with a blue background. It shows a single encoder with a label \"Predictions\" and \"Labels\" indicating that the model is not being trained.\n- **Trainable**: This section is labeled \"Trainable\" and is shown with a red background. It also features a single encoder, but this time with a label \"Predictions\" and \"Labels,\" indicating that the model is being trained.\n- **Loss**: This section is labeled \"Loss\" and is depicted with a green background. It shows a single encoder with a label \"Predictions\" and \"Labels,\" indicating that the model is being trained and the loss is being calculated\n\nheterogeneous properties, such as sampling rate, timestamp, or duration, that increase the likelihood of capturing asynchronous events. For example, in vibration sensing applications (machine monitoring, vehicle detection), multimodal sensors (geophone, microphone, thermometer, etc.) often operate at different sampling rates, leading to temporal misalignments that require manual calibration [40]. Second, raw signals often lack intuitive interpretability. Unlike images or text, where visual",
            " train deep learning-based segmentation models [6]. The manual annotation process is both expensive and time-consuming due to the complexity of different modalities and anatomical structures [42], [43]. Moreover, a significant portion of medical images is acquired in three-dimensional formats, including MRI, CT, and video-based imaging (e.g., colonoscopy and ultrasound), further increasing the annotation workload.\n\nTo address these challenges, various approaches have been developed, which can be broadly categorized into unsupervised, supervised, and interactive methods. Unsupervised methods focus on learning spatial relationship between adjacent slices in 3D data, enabling label propagation from a single annotated slice to the entire volume during inference [8], [11]. However, these methods are prone to error drift [11]. Supervised methods often utilize few-shot learning [44], [45], where a model is trained on a small, fully annotated subset of data before being applied to the remaining data. This category involves 2D [22], [46], 3D [47], [48], and VOS models [12], [41], with VOS technique sometimes requiring minimal annotations during the inference stage. Interactive annotation methods integrate automated predictions with realtime human corrections, enabling quick refinement of initial annotations, and thus improving annotation efficiency [44].\n\n## III. METHODS\n\nThis work aims to develop robust and reliable slice propagation models that can accelerate the annotation of volum",
            "ad and learn from input data to make autonomous decisions to achieve particular  goals.  Still,  it  is  not  a  comprehensive  threat  to  replace  human  physical  presence.  Machine learning  (ML)  is  a  subfield  of  AI  that  uses  statistical  methods  to  acquire  knowledge  through  or independently of explicit programming [12], [13], [14] . ML employs algorithms to spot patterns in alreadyorganized data [15], [16], [17], [18] . Supervised, unsupervised, and reinforcement learning are the three main sub-types of ML. Classification and regression methods are used to construct predictive models from input and output data while learning is being supervised. Disease diagnosis is an example of supervised ML's subgroup classification output.\n\nIn  contrast,  pharmaceutical  efficacy  and  adverse  drug  reaction  (ADMET:  Absorption,  Distribution, Metabolism, Excretion, and Toxicity) prediction is an example of supervised ML's subgroup regression output [19] . Feature discovery and data clustering based solely on input data are examples of unsupervised learning techniques [20] . Illness subtype discovery via clustering and disease target discovery with featurefinding  approaches  possible  with  unsupervised  ML [21] .  Reinforcement  learning,  often  known  as  the 'science of decision-making trial and error', is a method for optimizing performance in a particular setting by implementing decisions. The results of this type of machine learning can be used in various conte",
            "a [14]. These models are usually pretrained on large-scale dialogue text corpora and then fine-tuned for intent detection utilizing different pretraining tasks. For example, IntentBERT [10] introduces supervised pretraining, which simultaneously optimizes a language modeling loss and a supervised loss on a small labeled dataset through a standard intent classification task. Shnarch et al. [11] also adopt a classification task on top of BERT; however, instead of relying on manually labeled intent dataset, they generated pseudo-labels through unsupervised clustering of query texts. LLM-Aug [15] further generates pseudo-labeled query texts by lever-\n\naging powerful large language models. In addition to taskoriented supervised pretraining, contrastive learning-based pretraining with data augmentation is also often employed with PLMs to tackle the few-shot intent detection problem. Specifically, CPFT [13] introduces a masked text augmentation strategy, treating a user query and its masked version as positive samples, and then pretrains a BERT encoder by bringing their representations closer through contrastive learning. Zhang et al. [12] adopt a similar approach by introducing contrastive loss as a regularization technique to optimize the query representation space. However, they rely on a different data augmentation strategy borrowed from SimCSE [16], where the same query text with different dropout ratios is treated as positive samples.\n\nNevertheless, most existing methods have ",
            "arning of a set of linear parametric CBFs from human demonstrations. In [8], the authors present an approach to synthesize local valid CBFs for control-affine systems with known but nonlinear dynamics. The expert demonstrations contain only safe trajectories collected with a fixed nominal controller.\n\nOut-of-distribution Analysis. Out-of-distribution (OOD) analysis is an emerging topic of machine learning that examines the distribution shifts where test data diverges from the training data distribution [15]. Unsupervised representation learning methods focus on learning domain-agnostic features from unlabeled data [20], [21], [22], [23]. However, these methods can introduce bias, if the OOD domain distributions overlap with the unlabeled data distribution [24]. Supervised learning methods incorporate implicit domain labels from both in-distribution and OOD data [25], [26]. While these methods are often more accurate due to the additional information, they may not generalize well to OOD examples that differ significantly from those seen during training.\n\n## III. PRELIMINARY\n\nWe consider ego-robots with underlying dynamics Àô x ( t ) = f ( x ( t ) , u ( t )) where x ( t ) takes values in an n -dimensional state space X ‚äÜ R n , u ( t ) ‚àà U ‚äÜ R m is the control vector, and f : X √ó U ‚Üí X is a Lipschitz-continuous vector field. We allow the dynamics function f to be generally nonlinear and not control-affine. Consider an unsafe region of the state space X u ‚äÇ X where safety constrai"
        ]
    },
    {
        "answer": "There is no information on this question in the provided documents.",
        "retrieval_context": [
            "ailed description of the image:\n\n1. **Input Layer**:\n   - The input layer is represented by the variable \\( x_t \\). This is the initial data fed into the network.\n\n2. **Hidden Layers**:\n   - The diagram shows four hidden layers, each containing a set of operations. These layers are labeled as \\( \\sigma \\), \\( \\sigma \\), \\( \\tanh \\), and \\( \\sigma \\). Each of these layers is connected to the previous and next layers.\n   - The first hidden layer is connected to the input layer and the second hidden layer.\n   - The second hidden layer is connected to the first hidden layer and the third hidden layer.\n   - The third hidden layer is connected to the second hidden layer and the fourth hidden layer.\n\nAlthough RNN-based models are effective for handling sequential data and dynamic environments, a significant challenge in scaling quantum RNNs lies in the need to compute gradients through deep computational graphs, a process known as backpropagation-through-time (BPTT). An alternative approach to harness the potential of quantum RNNs without the computational overhead of BPTT is to treat RNNs as reservoirs . These quantum RNN reservoirs are randomly initialized and remain untrained. While this approach significantly reduces the required training resources, numerical studies have shown that QLSTM reservoirs can achieve performance comparable to fully trained QLSTMs in certain reinforcement learning tasks [15]. An alternative approach for building QML models capable of capturing temporal",
            "r and the output layer. The diagram is structured in a way that it shows the flow of data from the input layer to the output layer, with various layers in between.\n\n### Detailed Description:\n\n1. **Input Layer**:\n   - The input layer is represented by a series of nodes, each labeled with a unique index. These nodes are connected to the input layer, which is typically the source of data for the neural network.\n\n2. **Hidden Layers**:\n   - The diagram shows multiple hidden layers, each containing a series of nodes. These nodes are interconnected with arrows, indicating the flow of data from one layer to the next.\n   - The nodes in the hidden layers are connected to the input layer and the output layer.\n   - Each node in the hidden layers is connected to every node in the previous layer, forming a fully connected\n\nat time t , its action can be recursively expressed as:\n\n$$a _ { t } ^ { 1 } \\sim \\pi _ { t } ^ { 1 } \\left ( \\cdot | \\widehat { o _ { t } ^ { 1 } } \\right )$$\n\n$$a _ { t } ^ { i } \\sim \\pi _ { t } ^ { i } \\left ( \\cdot | \\widehat { o _ { t } ^ { 1 \\colon i } } , a _ { t } ^ { 1 \\colon i - 1 } \\right ) , i = 2 , \\cdots , n$$\n\nwhere œÄ i t is the policy distribution of agent i in time step t , ÃÇ o 1: n t is the global observation embeddings. The decision of the following agent depends on the decisions of its previous agents.\n\nEach agent aims to maximize its expected return, namely the value function, by optimizing its policy distribution given its predecessors' strategies.",
            "s with a layer labeled \"Input Layer\" which is the initial layer that receives input data.\n   - **Layer 2**: This layer is labeled \"Linear Layer\" and is followed by a \"Convolutional Layer.\"\n   - **Layer 3**: This layer is labeled \"Average Pooling Layer\" and is connected to the previous layer.\n   - **Layer 4\n\nproclivities will be reflected within the hidden representations outputted by each layer of the backbone block. Concretely, the MPR module takes in two input tensors: the decoupled fluctuation pattern group { E n k ‚àà R B ¬∑ d √ó P L √ó P d } k ‚àà [1 ,K ] output by the current layer's adapter block and the hidden representation H n -1 ‚àà R B ¬∑ d √ó P L √ó P d generated by the previous layer's backbone block (for the first MPR module, it is the embedding E 0 ‚àà R B ¬∑ d √ó P L √ó P d of the observed sequence). During the forward progression, the distinctive zero-convolution design ensures that MPRs are inclined to learn inductive biases from diverse fluctuation patterns. Specifically, within the shallow MPR block, the high-frequency pattern { E n k } k ‚àà [2 ,K ] undergoes a 1 √ó 1 convolution with both weights and biases initialized to zero, and then is concatenated with the low-frequency pattern { E n k } k =1 into the mixed fluctuation pattern tokens E n concat . Subsequently, similarly through the self-attention mechanism, and finally through the pooling layer, it is combined with the hidden representation H n -1 via element-wise addition as the prompt embedding P n ‚àà R B ¬∑ d √ó P L √ó",
            "esses fewer tokens than the encoder/decoder part, achieving our goal of token number compressing.\n\nWe assume that the encoder/decoder and middle parts work in different levels of granularity, for example, the encoder/decoder part should be responsible for concrete tasks, focusing on few tokens, while the middle part should take on abstract tasks, involving long-range context. So we used different configurations for them respectively. On the one hand, we use local attention in the encoder/decoder part. On the other hand, we removed the positional encoding in the middle part.\n\nFigure 1: The architecture of Synergy\n\nThe image consists of three diagrams, labeled (a), (b), and (c), which depict different aspects of a neural network architecture. Here is a detailed description of each diagram:\n\n### Diagram (a) - The Overview\nThis diagram provides a high-level view of the neural network architecture. It includes the following components:\n\n1. **Input Layer**: This is the initial layer that receives the input data.\n2. **Encoder**: This layer processes the input data and transforms it into a more abstract representation.\n3. **Residual Connections**: These are skip connections that help in training deep networks by allowing the gradient to be directly backpropagated to earlier layers.\n4. **Decoder**: This layer takes the abstract representation from the encoder and reconstructs the original input data.\n5. **Output Layer**: This is the final layer that produces the output of the network.",
            "is a diagram of a neural network architecture. It consists of various layers and components, each represented by different colors and shapes. Here is a detailed description of the image:\n\n1. **Input Layer**:\n   - The input layer is located at the bottom left of the diagram. It is represented by a series of horizontal lines with different colors. The colors indicate different input features.\n\n2. **Convolutional Layers**:\n   - The diagram shows multiple convolutional layers, which are represented by rectangles with a grid-like pattern. These layers are responsible for extracting features from the input data.\n   - The convolutional layers are stacked vertically, with each layer processing the output of the previous layer.\n\n3. **Activation Functions**:\n   - After each convolutional layer, there is an activation function applied. These are represented by small rectangles with a specific color.\n   - The activation functions are crucial for introducing non-linearity into the\n\nIn [59], a DL framework was developed for osteoporosis classification of hip radiographs, further examining whether structured clinical covariates could enhance image-based predictions. The dataset comprised 1,131 radiographs from patients aged 60 years or older, each paired with DXA-derived T-scores. Five pre-trained CNNs, ResNet18/34, GoogleNet, EfficientNet-b3, and EfficientNet-b4, were fine-tuned on manually cropped hip regions that mimic  DXA  measurement  fields.  To  integrate  clinical  context,  an  en",
            "ppears to be a diagram of a neural network architecture, specifically a variant of the Echo State Network (ESN) architecture. The diagram is structured in a way that shows the flow of information through the network, with various layers and connections. Here is a detailed description of the image:\n\n### Description:\n\n1. **Input Layer**:\n   - The input layer is at the top left of the diagram. It consists of a single neuron, which is connected to the input data.\n\n2. **Hidden Layers**:\n   - The diagram shows multiple hidden layers, each containing several neurons. These layers are interconnected with arrows indicating the flow of information.\n   - The neurons in the hidden layers are organized in a grid-like structure, with each neuron connected to its immediate neighbors.\n   - The connections between neurons in the hidden layers are represented by arrows, with different colors indicating different types of connections.\n\n3. **Output Layer**:\n\nground formula as:\n\n$$\\begin{array} { r l r } { E m b _ { j } ( h ) } & { = } & { F _ { j } ( M _ { n e } ( h , 1 ) \\to h , \\dots , M _ { n e ( h , d _ { j } ) \\to h } ) } \\\\ { o _ { j } ( h ) } & { = } & { G _ { j } ( M _ { n e ( h , 1 ) \\to h } , \\dots , M _ { n e ( h , d _ { j } ) \\to h } ) } \\end{array}$$\n\nwhere M j b ‚Üí h is the message defining the different neuralsymbolic methods, F j , G j are generic functions aggregating back the encoded representation and the output of a node, ne ( h, 1) , . . . , ne ( h, d j ) are the neighbours o",
            "uts Y 1 , . . . , Y m .\n\nThe image represents a diagram of a neural network, specifically a binary classification neural network. The diagram is structured in a way that it shows the input layer, the hidden layer, and the output layer. Here is a detailed description of the components and their connections:\n\n1. **Input Layer**:\n   - The input layer consists of four nodes, labeled as \\( X_1, X_2, X_3, X_4, \\ldots, X_n \\).\n   - Each input node is connected to every neuron in the hidden layer.\n\n2. **Hidden Layer**:\n   - The hidden layer is composed of a series of interconnected neurons, each represented by a circular node.\n   - There are \\( n \\) neurons in the hidden layer, where \\( n \\) is the total number of input features.\n   - Each neuron in the hidden layer is connected to every neuron in the input layer.\n\n3. **Output Layer**:\n   -\n\nThe following crucial observation, rooted in the universal approximation capabilities of ANNs [16], shows that the operation of an arbitrary binary ANN with multiple outputs can be reduced to a collection of binary ANNs, each with a single output.\n\nRemark 23. The input-output relationship of any binary ANN with multiple outputs can be emulated by means of a collection of binary ANNs, each with a single output.\n\nGuided by Remark 23, and to simplify the presentation, we shall restrict our analysis to singleoutput binary ANNs.\n\nFirst, note that a binary ANN with a single binary output Y induces a Boolean function f , which we refer to as the Boolean",
            "sents the accuracy, with a scale from 0.0 to 1.0.\n\nThere are two lines on the graph:\n1. A blue line labeled \"test_acc\"\n2. A red line labeled \"train_acc\"\n\nThe \"test_acc\" line starts at a lower value on the y-axis, close to 0.0, and shows a gradual increase as the number of epochs increases. The \"test_acc\" line remains relatively flat, indicating that the accuracy does not significantly change over the epochs.\n\nThe \"train_acc\" line starts at a higher value on the y-axis, close to 1.0, and also shows a gradual increase. This line is more steep\n\nFigure 13: L=3, 5-step reasoning\n\nThe image is a graphical representation of a neural network, specifically a multi-layer perceptron (MLP) with three layers. The neural network is depicted with nodes (or neurons) connected by lines, which represent the connections between the layers. Here is a detailed description of the image:\n\n### Description:\n\n1. **Layers**:\n   - **Layer 0**: This is the input layer. It has 35 neurons, each connected to every neuron in the next layer.\n   - **Layer 1**: This layer has 24 neurons. Each neuron in Layer 1 is connected to every neuron in Layer 0.\n   - **Layer 2**: This layer has 90 neurons. Each neuron in Layer 2 is connected to every neuron in Layer 1.\n   - **Layer 3**: This is the output layer. It has 3 neurons. Each neuron in Layer\n\nAs shown in Figure 13, when transformer produces a correct answer in the 5-step reasoning task, the attention and residual connections do not conform to the expected reasonin",
            " the process of training a neural network for a specific task, likely involving image or signal processing. Here is a detailed description of the image:\n\n1. **Neural Network Architecture**:\n   - The diagram shows a neural network with multiple layers.\n   - The input layer consists of two nodes: one for coordinates (x, t) and another for control (u).\n   - The hidden layers are represented by a series of interconnected nodes, forming a complex structure.\n   - The output layer is represented by a single node, which is connected to the input layer.\n\n2. **Optimization Step**:\n   - The optimization step is depicted as a process that occurs after the forward pass of the neural network.\n   - The optimization step involves calculating the gradient of the loss function with respect to the weights of the network.\n   - The gradient\n\nfor each ODE or PDE. Section VI concludes the study.\n\n## II. RELATED WORK\n\nIn recent years, PINNs have been widely used to solve differential equations [10], [11], [12], [13], [14]. An extension of PINNs for solving optimization problems was performed by Lu et al. [15], where the authors used a PINN and a control network for topology optimization for inverse design problems. They trained a neural network on a common loss function consisting of the residual of the differential equation and the cost function of the topology optimization problem. Molawi et al. [5] extended the idea of a control neural network and a PINN to optimal control problems. Song et al. ["
        ]
    },
    {
        "answer": "A convolutional neural network is built from a sequence of distinct building blocks:\n\n* **Input layer** ‚Äì receives the raw image (or other grid‚Äëstructured data).  \n* **Convolutional layers** ‚Äì apply learnable filters to extract local spatial features from the input (e.g., Conv2D layers).  \n* **Non‚Äëlinear activation functions** ‚Äì such as ReLU, applied after each convolution to introduce non‚Äëlinearity.  \n* **Pooling (or down‚Äësampling) layers** ‚Äì reduce the spatial resolution while retaining the most salient information (e.g., max‚Äëpooling).  \n* **Fully‚Äëconnected (dense) layers or global‚Äëpooling layers** ‚Äì combine the learned feature maps into a final representation for tasks such as classification or regression.  \n\nAdditional components that often appear in CNN diagrams include **batch‚Äënormalization layers** and **skip connections**, but the core components are the input, convolutional + activation blocks, pooling/down‚Äësampling, and the final fully‚Äëconnected (or global‚Äëpooling) stage.",
        "retrieval_context": [
            "ty as:\n\n$$p ( x ) = \\prod _ { t = 1 } ^ { T } p ( x _ { i } \\, | \\, x _ { 1 } , x _ { 2 } \\dots , x _ { t - 1 } ; \\theta ) ,$$\n\nwhere p ( x t | x 1 , x 2 , . . . , x t -1 ; Œ∏ ) represents the probability of the current elements x t conditioned on all previous\n\nFig. 6: A general illustration of model backbones: CNN, GNN, and Transformers.\n\nThe image is a diagram that outlines three different methods for image processing, specifically Convolutional Neural Networks (CNN), Efficient Graph Neural Networks (EGNN), and Self-Attention. Each method is represented by a distinct section within a structured diagram.\n\n### I. Convolutional Neural Network (CNN)\n\n- **Description**: The diagram starts with a simple CNN architecture. It consists of three main components:\n  - **Input Layer**: This is the initial layer that receives the input image.\n  - **Convolutional Layer**: This layer applies a set of filters to the input image to extract features.\n  - **Pooling Layer**: This layer reduces the spatial dimensions of the feature maps to decrease the computational complexity.\n  - **Fully Connected Layer**: This layer connects every neuron in the previous layer to every neuron in the current layer.\n\n- **Diagram Representation**:\n  - The input image is fed into the convolutional layer.\n  - The output from the convolutional layer\n\nelements in the sequence, with Œ∏ denoting the model parameters. This property makes them highly effective for tasks such as language modeling, where predicting the next ",
            "class are generated using the diffusion model, we proceed to describe the architecture of the U-Net model, which is not only part of the reverse process of our diffusion model but also used for segmenting the brain tissue region from\n\nFigure 3: The illustration depicts the architecture of a U-Net, which has four main components: the encoder, bottleneck, decoder, and skip connections. The encoder consists of five convolutional blocks (C1 to C5), progressively reducing spatial dimensions from 128√ó128 to 8√ó8 while increasing the feature channels from 32 to 512. Each block uses convolutional operations (blue) with 3√ó3 kernels, ReLU activations, and 'same' padding (purple), followed by max-pooling layers (red arrows) for downsampling. The bottleneck operates at the lowest resolution (8√ó8) with the highest abstraction level (512 channels). The decoder upscales feature maps back to 128√ó128 using transposed convolutions (green arrows) and convolutional blocks (C6 to C9) while reducing feature channels. Skip connections (gray arrows) link corresponding layers of the encoder and decoder, ensuring that fine-grained spatial details are preserved. Finally, a 1√ó1 convolution (yellow) generates the output segmentation map (128√ó128√ó1).\n\nThe image is a diagram of a neural network architecture. It is a detailed representation of a deep learning model, specifically a convolutional neural network (CNN), which is commonly used for image processing tasks. The diagram is structured in a hierarchica",
            "nent connected to the next in a sequential manner. Here is a detailed description of the components and their connections:\n\n1. **Image**: The topmost component in the diagram is the input image. This is the raw data that the CNN will process.\n\n2. **Conv2D(3, 3)**: This is the first convolutional layer. The parameters \"3, 3\" indicate that the layer uses 3 filters (or kernels) of size 3x3. This layer is responsible for extracting features from the input image.\n\n3. **BN (Batch Normalization)**: Following the convolutional layer, there is a Batch Normalization (BN) layer. This layer normalizes the activations of the previous layer at each batch, which helps in stabilizing\n\nThe  architecture  is  expressed  mathematically as:\n\n$$h ( X ) = G _ { 1 } ( H _ { 1 } ( X ; \\theta _ { 1 } ) ; \\theta _ { 2 } ) \\\\ + G _ { 2 } ( H _ { 2 } ( X ; \\theta _ { 3 } ) ; \\theta _ { 4 } )$$\n\n$$F ( X ) = G _ { 3 } ( H _ { 3 } ( h ( X ) ; \\theta _ { 5 } ) ; \\theta _ { 6 } )$$\n\nwhere ùêπ denotes the DPN, ùëã denotes the input signal, ùêª1 and ùêª2 represent the distinct pathways, ùê∫1 and ùê∫2 denote the transformation functions,  and ùúÉ represents  the  parameters  of the respective layers. The primary pathway, ùêª1 , is architected to extract fine-grained, localized features,  while  the  auxiliary  pathway, ùêª2 ,  is geared  towards  encapsulating  broader,  global features.  A  more  complete  representation  of features  is  possible  due  to  the  dual-pathway design.\n\n## 3.3. Self-Attentive Canonical Correlation",
            ") + H _ { t - 1 } ,$$\n\nwhere F ( ¬∑ ) denotes the ConvLSTM transformation. This additive update structure enhances long-term memory retention without amplifying gradient decay across layers.\n\nFrom a physical modeling perspective, this can be seen as enforcing continuity in fea-\n\nture space analogous to the conservation of charge or mass in electrochemical systems. By allowing the unaltered state to flow through the network, residual ConvLSTM better tracks slowly evolving interfaces and preserves spatial gradients critical for resolving dendrite tip morphology and ion depletion zones.\n\nIn this work, we integrate residual connections within stacked ConvLSTM layers, allowing the network to maintain hierarchical representations of spatiotemporal patterns. This design enables efficient learning of long-term dendrite growth dynamics under varying electrochemical conditions.\n\nFigure 2: (Left) ConvLSTM architecture for spatiotemporal modeling. (Right) Residual connection mechanism to enhance information retention.\n\nThe image depicts a neural network architecture, specifically a convolutional neural network (CNN) designed for image processing tasks. The architecture consists of two main components: a weight layer and a convolutional layer.\n\n### Description of the Image:\n\n1. **Input Layer (X)**:\n   - The input to the network is represented by the variable \\( X \\). This could be an image or a set of features extracted from an image.\n\n2. **Weight Layer**:\n   - The input \\( X \\) is fed int",
            " (CNN) architecture. The architecture is composed of several layers, each with specific functions. Here is a detailed breakdown:\n\n1. **Inputs**: The input layer is at the top, which receives the input data.\n2. **Convolutional Layers**: There are multiple convolutional layers (Conv2D) stacked on top of each other. Each convolutional layer is followed by a layer normalization (Layer Normalization) and a ReLU (Rectified Linear Unit) activation function.\n3. **Pooling Layers**: After the convolutional layers, there is a pooling layer (C). This layer performs downsampling by taking the maximum or average value across a window of the input.\n4. **Output Layer**: The final layer is the output\n\nFigure 5: Structure of diffferent convolutional blocks' structure\n\nThe image is a detailed diagram of a convolutional neural network (CNN) architecture. The diagram is divided into three main sections, each representing a different block of the network. The blocks are labeled as Block-1, Block-2, and Block-3, and Block-4. Each block contains a series of layers, which are described in detail below.\n\n### Block-1:\n- **Conv2D Layer**: This is the first layer in the block. It uses 128 filters with a stride of 3 and a kernel size of 3x3.\n- **Max Pooling Layer**: This layer follows the Conv2D layer. It uses a pool size of 2x2.\n- **Conv2D Layer**: This layer uses 64 filters with a stride of 1 and a kernel size of 3x3.\n- **Max Pooling Layer**: This layer follows the Conv2D layer. It uses a pool\n\n## 3.5 H",
            "ocess. Here is a detailed description of the components and their functions:\n\n1. **Input Layer**:\n   - The input layer is not explicitly shown in the diagram but is implied to be the starting point for the neural network.\n\n2. **Encoder**:\n   - The encoder is a crucial component that processes the input data. It is divided into three main parts:\n     - **Visual Encoder**: This part processes visual data. It includes components like \"Feed Forward\" and \"Self-Attention.\"\n     - **Textual Encoder**: This part processes textual data. It includes components like \"Feed Forward\" and \"Self-Attention.\"\n     - **Multi-modal Encoder**: This part integrates both visual and textual data. It includes components like \"Feed Forward\" and \"Self-Attention\n\n[247, 372] and multi-task adaptation [522, 255], broadening their applicability to a variety of practical contexts. In Figure 3, we illustrate the parallel adapter and unified adapter structures.\n\nFigure 4: Comparison of serial adapter integration in Transformer architecture (left) and adapter layer structure (middle) and Hybrid adapter architecture(right)\n\nThe image depicts a detailed diagram of a neural network architecture, specifically focusing on a model that combines various components to process input data. The diagram is structured in a way that it is divided into three main sections, each representing a different part of the model.\n\n### Left Section:\n1. **Layer Norm**: This is a normalization layer that applies a layer-wise normalizati",
            "s2 :  (2,  2)  stride;  Dw-Conv:  Depth-wise separable convolution\n\nThis model, herein referred to as the rECGnition_v2.0 (robust ECG abnormality\n\nFigure  3 :  Structural  diagram  of  proposed  rECGnition_v2.0  model.  (A)  represents  the  ECG  heartbeat processor unit which was implemented using ConvB and Dw-ConvB modules, (B) represents the internals of the ConvB modules and (C) represents the internals of Dw-ConvB module where n is number of filters, k is kernel size, and s is stride size, (D) represents the meta data processor unit. The output from (A) and (D) is fed into SACC layer represented by (E). Finally, the output of SACC is forwarded to (F) which is the meta classifier responsible for predicting the probabilities of corresponding arrhythmia classes.\n\nThe image is a detailed diagram of a neural network architecture, specifically a convolutional neural network (CNN) designed for image classification tasks. The diagram is structured in a hierarchical manner, with various components and layers connected in a specific way. Here is a detailed description of the image:\n\n### Overall Structure\nThe diagram is divided into several sections, each representing a different part of the CNN architecture. The main components include:\n\n1. **Input Layer**: This is the initial layer that receives the input data. The input data is typically a 2D image represented as a matrix.\n\n2. **Convolutional Layers**: These layers perform convolution operations on the input data. The convolutio",
            "er is designed to progressively reduce the spatial dimensions of the input image while increasing the number of feature channels. In our architecture, the input to the network is a 128√ó128 grayscale image, which is processed through a series of convolutional layers. Each convolutional block in the encoder consists of two 3√ó3 convolution operations with 'same' padding, followed by a ReLU activation function to introduce non-linearity. Downsampling is achieved using max-pooling layers with a 2√ó2 kernel size, which reduces the spatial dimensions while doubling the number of feature channels. The encoder consists of five convolutional blocks (C1 to C5 in Figure 3), where the number of filters progressively increases from 32 to 512, and the spatial size decreases from 128√ó128 to 8√ó8. This hierarchical structure ensures that the network learns abstract and discriminative features while effectively compressing the input image.\n\n## Bottleneck\n\nThe bottleneck serves as the critical transition point between the encoder and decoder. At this stage, the feature maps achieve the highest level of abstraction while the spatial resolution is at its minimum. In our case, the bottleneck consists of feature maps with a resolution of 8√ó8 and 512 feature channels, representing a dense and compact abstraction of the input image. This layer captures semantic features that are essential for accurate segmentation while retaining sufficient information for reconstruction during the decoding phase.\n\n## ",
            " have emerged as a computationally efficient alternative in science and engineering (Thuerey et al., 2021; Zhang et al., 2023; Brunton et al., 2020), impacting e.g., weather forecasting (Kurth et al., 2023; Bi et al., 2023; Lam et al., 2023; Nguyen et al., 2023; Bodnar et al., 2024), protein folding (Jumper et al., 2021; Abramson et al., 2024), material design (Merchant et al., 2023; Zeni et al., 2025; Yang et al., 2024), and multi-physics modeling Alkin et al. (2024b). All these success stories share the common thread of deep learning surrogates unlocking new possibilities to overcome seemingly insurmountable challenges (Brandstetter, 2024).\n\nFigure 1: Overview of the model architecture and n-dimensional attention. Left: Our 5D SwinUNet with two down/upsampling stages. We indicate shapes at each resolution change. Right: The locality of the n-dimensional shifted window attention for nDWin-MSA (top) and nDSWin-MSA (bottom), illustrated as a 2D plane of 3D window-partitioned tokens. Connected components are highlighted as colored tokens in the 3D blocks, and as lines connecting them across dimensions.\n\nThe image appears to be a diagrammatic representation of a neural network architecture, specifically a variant of a convolutional neural network (CNN) designed for image processing tasks. The diagram is divided into two main sections: the left side shows the architecture of the network, while the right side provides a visualization of the network's output.\n\n### Left Side: Archit"
        ]
    },
    {
        "answer": "**Reinforcement learning (RL)** is a machine‚Äëlearning paradigm in which an **agent** interacts with an environment, observes a state, selects an action, and then receives a **scalar reward** (or punishment) that may be delayed. By repeatedly experiencing these state‚Äëaction‚Äëreward sequences, the agent learns a **policy**‚Äîa mapping from states to actions‚Äîthat aims to maximize the cumulative (discounted) reward over time. RL therefore focuses on **sequential decision‚Äëmaking**, balancing exploration of new actions with exploitation of known good actions.\n\n**How it differs from supervised learning**\n\n- **Supervised learning** trains a model on **labeled data**: each input example is paired with a known target (class label or regression value). The model learns to predict these targets directly from the patterns it discovers in the training data.  \n- **Reinforcement learning** does **not** rely on pre‚Äëlabeled examples. Instead, it learns **through trial‚Äëand‚Äëerror** using the reward‚Äëand‚Äëpunishment feedback from the environment. The goal is not to predict a given label but to discover a strategy that yields the highest long‚Äëterm reward.\n\nThus, while supervised learning is about learning a direct input‚Äëto‚Äëoutput mapping from static, labeled datasets, RL is about learning how to act in an environment over time by maximizing cumulative reward signals. (Sources: excerpts describing supervised learning and reinforcement learning in the provided documents.)",
        "retrieval_context": [
            "ine a policy or an optimal path) or to solve specific tasks (e.g: object detection and classification). Based on the way algorithms learn from data, it can be divided into three learning paradigms: supervised learning , unsupervised learning , and reinforcement learning . The primary differences among those three are in the types of result being produced, and the way neural network models validate and calibrate those results. Figure 1 illustrates such differences of those learning paradigms.\n\n- Supervised learning involves training a model from labeled data. In other words, a neural network learns to predict the future output based on what happened in the past input. It will identify any patterns or trends (also known as weight ) that are historically correlated with the ground-truth outcomes and then use them to perform future prediction.\n- Unsupervised learning analyzes and clusters unlabeled data sets; aiming to discover hidden patterns in data without the need for human intervention. This type of learning is mostly used for grouping data in a meaningful way, representing hight-dimensional data set in a low-dimensional form, and associating relationships between variables in a large dataset.\n- Reinforcement learning , in the other hand, does not rely on any pre-learned pattern. Instead, it mimics the trial-and-error learning process that humans use to achieve its goals. Specifically, neural network models in reinforcement learning use a reward-and-punishment paradigm as th",
            "on provides a brief background on reinforcement learning (RL) for researchers in the power systems community interested in its application to power network control (PNC). For a comprehensive overview of RL, we refer to the book by [28].\n\n## 1.3.1 Key concepts reinforcement learning\n\nReinforcement learning is a machine learning technique where an agent (the learner and decision maker) learns from experience how to make sequential decisions in an environment, guided by a reward (feedback).\n\nThe sequential decision problem is typically modeled as a Markov Decision Process (MDP), which consists of:\n\n- State space S . A state s t ‚àà S is the snapshot of the environment observed by the agent at time t , usually represented by a vector or matrix.\n- Action space A (or A s the set of available actions from state s ). After observing the state, the agent interacts with the environment at time t by taking an action (decision) a t ‚àà A according to its policy œÄ .\n- Transition function p : S √ó S √ó A ‚Üí [0 , 1] . The quantity p ( s t +1 | s t , a t ) describes the probability that action a t in state s t leads to state s t +1 .\n- Reward function r : S √ó A √ó S ‚Üí R . The quantity r ( s t , a t , s t +1 ) represents the reward received after transitioning from s t to s t +1 via action a t . In some settings, this reduces to r ( s t , a t ) or r ( s t ) .\n- Discount factor Œ≥ ‚àà (0 , 1) , which in a discounted MDP setting, determines the relative importance of future rewards.\n\nSection 1.4 presents ",
            "connected by arrows.\n   - The first layer has a series of nodes connected in a way that suggests a feedforward structure.\n   - The second layer has a series of nodes connected in a way that suggests a feedforward structure.\n\n## VII. QUANTUM REINFORCEMENT LEARNING\n\nReinforcement Learning (RL) is a fundamental paradigm in machine learning, wherein an autonomous entity, known as the agent , learns to make decisions by interacting with its environment through an iterative process. Although the concept is straightforward, RL has proven capable of solving complex sequential decision-making tasks, such as achieving human-level or superhuman performance in playing computer games. The agent interacts with a structured environment E over discrete time steps. At each time step t , the environment provides the agent with a state or observation s t . Based on this information, the agent selects an action a t from the available action set A according to a policy œÄ . The policy œÄ defines how the agent maps the observed state s t to an action a t . If the policy is stochastic, the action a t is sampled from a probability distribution œÄ ( a t | s t ) rather than being chosen deterministically. Once the agent executes the action a t , it transitions to a new state s t +1 and receives a scalar reward r t . This cycle repeats until the agent either reaches a terminal state or satisfies a predefined stopping condition, such as reaching a maximum number of steps. The sequence of interactions, begi",
            "                                                                            |  107 |\n|   6.4 | 3R2S: EF-OnTL setup across the three evaluated scenarios. . . . . . . . . .                                                                            |  125 |\n\n## 1 Introduction\n\nReinforcement Learning (RL) is a branch of machine learning that collects a set of algorithms which enable an intelligent agent to learn an optimal behaviour in an unknown task via trial and error [6]. Initially, RL was primarily used in academic settings to demonstrate the feasibility of learning a small unknown task through tabular representation or linear approximators.\n\nHowever, with the latest advancements in deep learning, RL has seen a sharp increase, as shown in Figure 1.1, where the number of RL publications has grown fourfold between 2017 and 2019, and this growth trend has continued thereafter. The figure shows the publication trend of RL-based research work when looking for \"Reinforcement Learning\" through Google Scholar search engine.\n\nIn the latest years, RL has been used to control agents in a variety of tasks, game-based or real-world tasks. Examples of applications include, robotics [11-13], financial [14, 15], asset management/allocation, large-scale optimisation such as, traffic [16-18], 5g networks [19, 20], medical decisions support [21-23], and natural language processing [24, 25].\n\nDespite the remarkable results collected in the aforementioned fields, RL still struggles to be efficie",
            " ‚úì                   | ‚úó            |\n\n## 3 Deep Reinforcement Learning\n\nRL [11,63,126] is a specialized area within AI. It equips an agent with the ability to learn in an environment by repeatedly interacting with it autonomously; and, importantly, it allows for the optimization of the agent's sequential behavior under uncertainty. In more detail, the agent observes the representation of the current state of an environment, takes an action that leads to a new state, possibly obtains a reward, observes the new current state, and so on. The agent's ultimate goal is to learn a policy, i.e. a mapping of environment states to actions, one which would maximize its cumulative reward.\n\nRL is naturally paired with the Markov Decision Processes (MDPs) framework [54,102,136], which provides a more detailed and rigorous portrayal of an evolving environment and its dynamics-albeit under certain assumptions, such as stationrity, that might or might not apply in the real world. MDPs are employed for modeling decision making problems, and are an extension of Markov Chains [62,107]. By definition, the MDP is a tuple ( S , A , T , R , Œ≥ ) where:\n\n- -S is a finite set of states\n- -A is a finite set of actions\n- -T ( s, a , s ‚Ä≤ ) = P r ( s ‚Ä≤ | s, a) is the transition function - a distribution that returns the probability of landing to state s ‚Ä≤ by doing action a in state s\n- -R ( s, a) is the reward function\n- -Œ≥ ‚àà [0 , 1] is the discount factor\n\nNoticeably, the transition probabilities and the",
            "nts.\n\n## 1 Introduction\n\nDeploying reinforcement learning (RL) (Sutton &amp; Barto, 2018) on physical robots, industrial processes, or in healthcare remains notoriously difficult for two reasons. First, exploration is both risky and data-hungry (Dulac-Arnold et al., 2019): A policy that begins from scratch can damage hardware, or user trust, long before gathering enough experience to learn. Second, reward mis-specification: even experienced domain experts often find it hard to translate informal task goals into a correct and safe numerical reward signal (Leike et al., 2018).\n\nA promising solution addresses both challenges simultaneously: It combines reward-free expert demonstrations with online preference-based feedback, allowing practitioners to leverage safe imitation learning while enabling corrective refinement through simple comparative judgments. This hybrid approach has achieved remarkable empirical success across domains. It is the core technique behind modern dialogue agents like ChatGPT, which is first trained to imitate curated demonstrations of desirable responses and then fine-tuned via RL from human feedback (RLHF) (Ouyang et al., 2022). Similar approaches have achieved near-expert performance in complex games like Atari and control tasks like MuJoCo (Christiano et al., 2017) and enabled safe real-world robot manipulation by ranking tele-operated clips before on-hardware fine-tuning (Brown et al., 2020). Our reward-free setting is distinct from a related line of",
            "g tasks at levels of expert humans (Achiam et al., 2023; Rafailov et al., 2023; Team et al., 2023; Touvron et al., 2023; Shao et al., 2024; Guo et al., 2025). While in the initial stage of training, known as pre-training , the trained model absorbs vast amounts of information, post-training techniques, such as reinforcement learning (RL), enable the model to develop preferable behaviors and expertise in specialized tasks (Sutton et al., 1998; Schulman et al., 2017; Christiano et al., 2017). The RL paradigm is very different from the popular predictive or generative learning paradigms (Shalev-Shwartz and Ben-David, 2014). While these try to either predict a label (Krizhevsky et al., 2017) or to reconstruct the data itself (Ho et al., 2020), RL does not set a clear target for the model. Instead, the model, by taking actions in response to presented scenarios, operates in an environment that sends the model feedback known as reward . RL algorithms configure the agent's behavior to maximize the reward. Thus, human preference-based rewards enable aligning LLMs with human preferences and values (Christiano et al., 2017; Ouyang et al., 2022; Achiam et al., 2023), and task rewards help LLMs improve in specific tasks (Lambert et al., 2024; Shao et al., 2024).\n\nNevertheless, while offering potential of gaining super-human breadth of skills (Silver and Sutton, 2025), RL does share the weakness of all machine learning paradigms, which is that of reliance on data. Although dispensing with",
            "ad and learn from input data to make autonomous decisions to achieve particular  goals.  Still,  it  is  not  a  comprehensive  threat  to  replace  human  physical  presence.  Machine learning  (ML)  is  a  subfield  of  AI  that  uses  statistical  methods  to  acquire  knowledge  through  or independently of explicit programming [12], [13], [14] . ML employs algorithms to spot patterns in alreadyorganized data [15], [16], [17], [18] . Supervised, unsupervised, and reinforcement learning are the three main sub-types of ML. Classification and regression methods are used to construct predictive models from input and output data while learning is being supervised. Disease diagnosis is an example of supervised ML's subgroup classification output.\n\nIn  contrast,  pharmaceutical  efficacy  and  adverse  drug  reaction  (ADMET:  Absorption,  Distribution, Metabolism, Excretion, and Toxicity) prediction is an example of supervised ML's subgroup regression output [19] . Feature discovery and data clustering based solely on input data are examples of unsupervised learning techniques [20] . Illness subtype discovery via clustering and disease target discovery with featurefinding  approaches  possible  with  unsupervised  ML [21] .  Reinforcement  learning,  often  known  as  the 'science of decision-making trial and error', is a method for optimizing performance in a particular setting by implementing decisions. The results of this type of machine learning can be used in various conte",
            ". Instead, it mimics the trial-and-error learning process that humans use to achieve its goals. Specifically, neural network models in reinforcement learning use a reward-and-punishment paradigm as they processes data. The algorithm constantly tries new things to learn and improving upon current approaches to maximize the defined reward.\n\nFig. 1. Three paradigms in deep learning: supervised, unsupervised, and reinforcement. (a) In supervised learning, models are trained on labeled data to mostly perform the task of classification (where results are categories) and regression (where results are continuous values); (b) in unsupervised learning, models are trained on unlabeled data to identify associated relationships in a dataset, insightful cluster data group, and reduce high dimensional data form.\n\nThe image appears to be a schematic diagram that outlines a process involving machine learning and decision-making. The diagram is divided into three main sections, each representing a different stage of the process.\n\n### Section (a):\n- **Title**: This section is labeled \"Labeled Data\" and shows a flow of data from left to right.\n- **Data**: The data is represented by a series of geometric shapes (circles, triangles, and rectangles) arranged in a specific pattern.\n- **Machine**: The machine is depicted as a series of interconnected gears, indicating a complex system.\n- **ML Model**: The machine is connected to a model, which is represented by a box with a gear symbol.\n- **Predictio"
        ]
    },
    {
        "answer": "Distributed systems maintain consistency by embedding explicit coordination and validation mechanisms into their runtime protocols.  In the documents, several approaches are described:\n\n* **Consensus‚Äëbased decision making** ‚Äì Protocols such as the *Decision Protocol* dynamically adjust consensus thresholds based on task criticality and system state, using voting and negotiation to reach agreement even when full unanimity is impossible.  This prevents deadlocks and ensures that updates become visible across agents in a coordinated fashion„ÄêDocument‚ÄØ1„Äë.\n\n* **Transaction‚Äëstyle guarantees** ‚Äì Traditional MAS designs integrate ACID‚Äëstyle transaction processing to provide strong, workflow‚Äëwide consistency and recovery.  Even when strict ACID is relaxed (as in SagaLLM), modular checkpointing, compensable execution, and validation steps are used to preserve consistency across multi‚Äëagent workflows„ÄêDocument‚ÄØ2„Äë.\n\n* **Redundancy and voting** ‚Äì Redundant execution of tasks or parallel agent runs, followed by aggregation through voting, scoring, or majority selection, mitigates stochastic failures and enforces a consistent outcome among divergent hypotheses„ÄêDocument‚ÄØ3„Äë.\n\n* **Guardrails and assertions** ‚Äì Proactive constraints (e.g., schema checks, semantic validators, behavioral assertions) are applied before or after execution to enforce syntactic and semantic consistency of agent outputs„ÄêDocument‚ÄØ4„Äë.\n\n* **Conflict‚Äëresolution and consistency management** ‚Äì Mechanisms such as quadruple operations, conflict‚Äëresolution policies, and consistency checks are employed to keep shared state coherent under high‚Äëconcurrency conditions„ÄêDocument‚ÄØ5„Äë.\n\nTogether, these mechanisms‚Äîdynamic consensus thresholds, voting/negotiation, transaction guarantees, redundancy, guardrails, and explicit conflict‚Äëresolution‚Äîconstitute the way distributed systems described in the provided documents handle consistency.",
        "retrieval_context": [
            "ation Protocol implements distributed decisionmaking mechanisms through a combination of voting systems and negotiation frameworks [2]. This protocol adapts consensus thresholds dynamically based on task criticality and system state, ensuring robust decision-making while maintaining system responsiveness. When proposals conflict, agents can resolve disagreements through negotiation, ensuring that progress is still made despite differences in opinion. Voting protocols allow agents to express preferences and reach a decision even when full consensus is not achievable, thus preventing deadlocks and ensuring that tasks continue to progress.\n\nThe Experience Management Protocol facilitates collective learning through structured logging and pattern extraction mechanisms [77]. Each agent logs its experiences and learning outcomes during task execution. These experience records may include successes, failures, the effectiveness of strategies used, and interactions with other agents. This protocol implements cross-agent knowledge sharing algorithms, enabling systematic improvement of system performance through accumulated experience.\n\nThe Credit Allocation Protocol addresses the challenge of fair contribution assessment through multi-level propagation mechanisms. Agents that participate in tasks receive corresponding credit based on their contributions [79]. By implementing task-specific metrics and performance-based distribution algorithms, this protocol ensures equitable reward alloc",
            "ns, and typically depends on manual intervention or simplistic algorithms, thereby limiting overall system adaptability and performance.\n- Insufficient intelligent interaction and data state sharing among agents: Many AI agents function in isolation, lacking mechanisms for cross-agent state synchronization and data sharing. This limitation hinders collaborative task execution and prevents seamless handovers between agents. Furthermore, these agents often possess constrained memory and storage capabilities, which makes it difficult to retain and share historical interaction records. As a result, achieving efficient data exchange, knowledge sharing, and task decomposition in distributed agent networks remains a pressing research challenge.\n\nTo address the challenges outlined above, this paper proposes a structured multi-agent workflow framework: Hierarchical Agent WorKflow (HAWK) . HAWK is designed to encompass all major aspects of an agentbased system, ranging from client-side workflow specification and submission, through task scheduling and execution, to resource management and provisioning. The framework consists of four functional layersUser Layer , Workflow Layer , Operator Layer , and Agent Layer -plus the Resource Layer , and defines sixteen key interfaces to ensure both interoperability and flexibility among its components. By decoupling responsibilities across layers, HAWK delivers a modular and independently evolvable architecture that enables developers to focus on ",
            "do-data generation and cross-task scenario simulation to evaluate transfer robustness.\n\nMoreover, Gao [15] presents a deep graph modeling framework for performance risk detection in structured queries, providing a graph-oriented perspective on semantic dependency modeling. This aligns with our emphasis on preserving cross-lingual structural consistency through alignment losses and feature normalization. Meanwhile, Peng [16] examines hallucination detection mechanisms based on evidence-aligned context modeling, which highlights the necessity of retaining original knowledge traces during adaptation-a principle embedded in our prompt injection and freezing strategy.\n\nIn the domain of system optimization and dynamic control, several studies contribute indirectly to the design of our training and adaptation routines. Sun et al. [17] introduce a deep Q-network framework for intelligent cache management, which reinforces the concept of reward-based update strategies in constrained environments, akin to our gradient control and task-awareness modules. Zhu et al. [18] employ graph neural networks to facilitate collaborative perception in distributed systems, providing insights into how feature interaction and message passing schemes can enhance representation robustness, a concept we apply during multilingual adaptation.Additionally, the self-attention-based multi-source metric modeling approach proposed by Xin and Pan [19] serves as a blueprint for our alignment of task-level and lan",
            "omate key tasks traditionally requiring hand-coded coordination logic, including state tracking, dependency analysis, log schema generation, and recovery orchestration. Although SagaLLM relaxes strict ACID guarantees, it ensures workflow-wide consistency and recovery through modular checkpointing and compensable execution. Empirical evaluations across planning domains demonstrate that standalone LLMs frequently violate interdependent constraints or fail to recover from disruptions. In contrast, SagaLLM achieves significant improvements in consistency, validation accuracy, and adaptive coordination under uncertainty-establishing a robust foundation for real-world, scalable LLM-based multi-agent systems.\n\n## PVLDB Reference Format:\n\nEdward Y. Chang and Longling Geng. SagaLLM: Context Management, Validation, and Transaction Guarantees for Multi-Agent LLM Planning. PVLDB, 14(1): XXX-XXX, 2025. doi:XX.XX/XXX.XX\n\n## PVLDB Artifact Availability:\n\nThe source code, data, and/or other artifacts have been made available at URL_TO_YOUR_ARTIFACTS.\n\n## 1 INTRODUCTION\n\nMulti-Agent Systems (MAS) have long been a cornerstone of distributed computing and database systems [15, 25, 28]. Over the past few decades, their development has followed two primary trajectories. In the database community, MAS traditionally integrated the foundational transaction processing principles, particularly ACID properties [21, 43], to ensure consistency and reliability for complex multi-step operations. For long-l",
            "etween followers and their respective leaders satisfies the following properties:\n\n1. When each agent in a opinion group collaborates with its leaders, the MAS tends to reach the stable consensus, converging to the average state of the leaders.\n2. When the leaders' average belief is higher than other agents' beliefs, the leaders with higher beliefs can expedite the convergence to the stable consensus.\n\n## 4 Methodology\n\nIteration until reaching full consensus\n\nor maximum iteration number\n\nThe image is a diagram that outlines a collaborative decision-making process involving multiple agents. The diagram is structured in a flowchart format, with various components and processes connected by arrows. Here is a detailed description of the image:\n\n1. **Agents**:\n   - The diagram starts with three groups of agents:\n     - **G1**: Represented by blue circles.\n     - **G2**: Represented by orange circles.\n     - **G3**: Represented by purple circles.\n\n2. **Opinion Groups**:\n   - Each group of agents is divided into opinion groups:\n     - **G1**: Divided into three opinion groups:\n       - **Dominant Consensus Group**: Represented by blue circles.\n       - **Most Uncertain Opinion Group**: Represented by orange circles.\n       - **Agent**: Represented by purple circles.\n     - **G2**: Divided into three opinion groups:\n       - **Dominant Consensus\n\nFigure 3: An illustration of the Belief-Calibrated Consensus Seeking (BCCS) framework. The arrows represent the workflows. After obtaining",
            "Policy &amp; Strategy Adaptation).\n\n- Redundancy &amp; Voting . Redundancy-based resolution introduces robustness by not relying on a single agent run or output. Instead, multiple agents or runs are executed in parallel or sequence, and their outputs are aggregated through voting, scoring, or selection mechanisms [22, 47, 110]. This ensemble-style redundancy mitigates stochastic failure modes, such as planning inconsistencies or semantic drift in generation, by comparing multiple hypotheses and selecting the most consistent or confident one. In practical systems, this may involve generating N candidate responses and reranking them using a reward model, or having multiple agents independently attempt a task and using majority voting to determine correctness. Redundancy is especially valuable for addressing emergent behavior anomalies, epistemic uncertainty, or brittle reasoning paths. It is also well-aligned with distributed agent systems or agent collectives, where parallelism is natural and cost-effective. Though computationally expensive, redundancy enhances reliability in high-stakes or noisy environments.\n- Guardrails &amp; Assertions . Guardrails and assertions serve as proactive safeguards that enforce constraints on agent behavior before or after execution. These include syntactic constraints (e.g., output must follow a JSON schema), semantic validators (e.g., the answer must reference a tool result), and behavioral assertions (e.g., the agent must always output a fina",
            "ccur every day and new content is continuously generated. The behavior of real users is rooted in these ever-evolving social contexts and policy agendas. However, the static knowledge of LLMs prevents them from aligning with the dynamic nature of the real-world social environment [2, 15]. There is a gap between the simulated context and the real world, which results in discrepancies between the simulation process and outcomes compared to those in reality. Therefore, it is necessary to establish an update mechanism to keep the simulated environment synchronized with the real world.\n\n## Q2. How to align simulated agents with target users precisely?\n\nThe composition of users in the real world is both complex and diverse, making it impractical to enumerate all users in every scenario. Therefore, it is essential to identify target users whose distribution aligns with that of the users in the corresponding scenario, thereby accurately reflecting the real-world composition and relationships [17, 45]. Based on this, precise target user simulation also requires providing agents with a detailed and comprehensive description of the corresponding users, often involving the integration of high-fidelity demographic, contextual, and behavioral data.\n\n## Q3. How to align the interaction mechanism with the real world among different scenarios?\n\nThe diversity of social interactions presents challenges in social simulation design, requiring deliberate choices regarding the number of individuals",
            "aintain balanced states without explicit rules, improving adaptability to long-term interactions (Xu et al., 2025). Moreover, consistency management approaches, including quadruple operations and conflict resolution, ensure coherent updates under high-concurrency requirements (Chhikara et al., 2025).\n\nWith respect to memory utilization , prior work has emphasized retrieval and application strategies that enhance effectiveness in reasoning and interaction. Hierarchical retrieval combined with heat-driven updating allows context-relevant access for personalized responses (Kang et al., 2025). Entity-relation dual-path retrieval methods jointly exploit symbolic and embedding-based representations to improve accuracy in complex queries (Chhikara et al., 2025). In addition, compression-based methods have been adopted to retain informativeness while reducing storage overhead, supporting scalability in extended interactions.\n\nDespite these advances, most memory systems remain task-agnostic , overlooking the integration of agent goals as priors. This leads to two challenges: limited specialization for heterogeneous tasks, and significant computational and storage costs when striving for broad generality. These limitations point to the need for task-oriented memory systems that explicitly incorporate agent objectives into representation, management, and utilization, thereby capturing task-relevant information more efficiently and adaptively.\n\n## 2.2 Retrieval Module for Memory System\n\n",
            "DP 5 links unit codes and symbols according to [18] to these type descriptions, ensuring system-wide compatibility.\n\nTo express interdependencies as mathematical relations, suitable formalisms have been integrated into the alignment ontology. Building on prior work [19, 20], Wenzel [5] introduced OpenMath-RDF as a representation of mathematical expressions in knowledge graphs. This provides the foundation for capturing parameter interdependencies. A systematic method to integrate these interdependencies into formalized process models was introduced in [21].\n\nAs shown in [7], this approach enables both the composition and decomposition of processes together with their associated interdependencies. To handle the resulting complexity across multiple levels of abstraction, filtering mechanisms were introduced to extract only context-relevant process data. This, in turn, revealed the need for dedicated consistency checks, which form the focus of the present work. The verification mechanisms proposed here are specifically tailored to the structural characteristics of the ParX model and are intended to assist engineers in the reliable application of reusable interdependencies within knowledge-based reasoning.\n\n## III. CONSISTENCY VERIFICATION APPROACH\n\nIn knowledge-based modeling environments, particularly those supporting design and configuration of CPS, consistency verification plays a central role [4]. As the amount and complexity of interconnected data increases in the semantic "
        ]
    },
    {
        "answer": "Batch normalization is used to keep the activations in each layer on a stable, uniform scale. By normalizing the inputs to a layer with the mean and variance computed over each mini‚Äëbatch, it:\n\n* Reduces internal covariate shift, so the distribution of layer inputs does not change dramatically during training.  \n* Helps prevent vanishing and exploding gradients by keeping activation ranges stable, which improves gradient flow.  \n* Introduces a small amount of randomness (because the statistics come from a mini‚Äëbatch rather than the whole dataset), acting similarly to dropout and thus helping to reduce overfitting.  \n\nOverall, batch normalization stabilizes and speeds up training while also providing a regularizing effect.",
        "retrieval_context": [
            "ing training-batch normalization ensures more uniform activations across the network.  This  consistency  helps  address  the  issues  of  vanishing  and  exploding  gradients  by  keeping  the activation ranges stable. Moreover, batch normalization introduces a level of noise by basing the normalization of inputs on the mean and variance from a mini-batch instead of the entire dataset. This randomness, similar to  the  effect  of  dropout, reduces overfitting  by  preventing the  model from relying too heavily on specific patterns in  the  training  data.  Through  the  adaptation  of  the  DEMNET  architecture  and implementing  key modifications tailored to our objectives, we aimed to attain promising results in AD detection while reducing the total and trainable parameters of the DEMNET model.\n\n## 3.6 Weight Initialization Strategy for IR-BRAINNET and Modified-Dement\n\nIn  neural  networks,  weights  are  essential  because  they  significantly  affect  the  network's  performance; therefore, considerable effort must be invested in determining their initial values and configurations [47]. The choice of initial weights in neural networks is crucial as it can help prevent issues such as gradient explosion or vanishing gradients. [48] In this paper, we used the Kaiming He weight initialization method to properly initialize the weights of all trainable layers in both CNNs, except for the second layer of IR-BRAINNET. According to [48], the formula for the Kaiming He initializer",
            " window and dense sliding window methods were applied to the windowed dataset, while the smart batching method was used on the granular dataset. Each method had its pros and cons, with the slide window method producing relatively good results but requiring more prior knowledge for correct predictions. Recurrent Neural Network with LSTM nodes and Transformer models were developed using these preprocessing methods; however, their performance was inferior to the baseline model. During the exploration phase, we noticed that the deep learning models predicted \"deterioration\" at the beginning but later predicted \"no deterioration,\" which conflicted with the label's actual meaning. Additionally, the imbalanced dataset posed challenges for accurate predictions, even with class weights.\n\nSmart batching is a new data processing method that is closer to real-time conditions. We found that simpler model structures worked better for smart batching. The next step is to improve smart batching methods by decreasing model complexity and adjusting parameters. Additionally, new model structures, such as multi-time attention networks for irregularly sampled time series, can be implemented to enhance performance. Furthermore, we suggest two new methods to address the problem of imbalance. The first is ensemble learning, in which multiple models are trained on different subsets of the imbalanced dataset and their predictions combined to obtain a final prediction. This approach can improve the mode",
            " and M. Welling, 'Auto-Encoding Variational Bayes,' University of Amsterdam, Amsterdam, Tech. Rep., 12 2013.\n- [12] D. J. Rezende and S. Mohamed, 'Variational Inference with Normalizing Flows,' Proceedings of the 32nd International Conference on Machine Learning , pp. 1530-1538, 5 2015.\n- [13] D. K. Foley and E. Scharfenaker, 'Bayesian Inference and the Principle of Maximum Entropy,' 7 2024.\n- [14] D. Ouyang, B. He, A. Ghorbani, N. Yuan, J. Ebinger, C. P. Langlotz, P. A. Heidenreich, R. A. Harrington, D. H. Liang, E. A. Ashley, and J. Y. Zou, 'Video-based AI for beat-to-beat assessment of cardiac function,' Nature , vol. 580, no. 7802, pp. 252-256, 4 2020.\n- [15] Y. N. Dauphin, A. Fan, M. Auli, and D. Grangier, 'Language Modeling with Gated Convolutional Networks,' 12 2016.\n- [16] S. Ioffe and C. Szegedy, 'Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift,' 2 2015.\n- [17] D. Hendrycks and K. Gimpel, 'Gaussian Error Linear Units (GELUs),' 6 2016.\n- [18] Y. Burda, R. Grosse, and R. Salakhutdinov, 'Importance Weighted Autoencoders,' 9 2015.\n- [19] A. Paszke, S. Gross, F. Massa, and et al., 'PyTorch: An Imperative Style, High-Performance Deep Learning Library,' 12 2019.\n- [20] A. Vaswani, N. Shazeer, N. Parmar, J. Uszkoreit, L. Jones, A. N. Gomez, L. Kaiser, and I. Polosukhin, 'Attention Is All You Need,' Google Brain, Tech. Rep., 2017.\n- [21] M. Beck, K. P¬® oppel, M. Spanring, A. Auer, O. Prudnikova, M. Kopp, G. Klambauer, J. Brandstett",
            "fference frames within a batch are normalized to the same scale. Equ.(2) demonstrates how the batch normalization layer learns optimal parameters that amplify pixel changes while simultaneously minimizing the impact of noise.\n\n$$N _ { k } ( t ) = \\frac { ( \\beta _ { t } * D _ { k } ( t ) + \\gamma _ { t } ) - \\mu D _ { k } } { \\sigma D _ { k } }$$\n\nWhere ùúá is mean and ùúé is standard deviation. Without this normalization step, the frame differences tend to be close to zero, given that subtle variations in user facial skin pixel values between consecutive frames are relatively small. Consequently, batch normalization not only amplifies numerical differences but also normalizes them. Figure 6 compares the contrasting pre-processing workflows of existing methods and ours.\n\n## 3.4 Fast Long-range Spatio-temporal Heart Rate Recognizing\n\n- 3.4.1 Primer on Long-range Spatio-temporal Model for HR Detection. Camera-based heart rate recognition involves mapping video sequences to signal sequences. Specifically, it translates the spatial dimension of light reflection in video frames into temporal variations of heart rate signals. As depicted in Figure 8, peak positions exhibit similar characteristics regardless of diverse user heart rate speed or amplitude. Our key observation is that these characteristics include abrupt changes in signal trends and relatively high signal intensity. Therefore, constructing a long-range spatio-temporal model can enhance detection accuracy and robustness.\n- ",
            "ield of deep learning is rapidly embracing quantization-aware training and low-bit inference, driven by hardware advancements like next-generation GPUs (e.g., GB200) offering native support for 4-bit computations. This promises significant efficiency gains for large-scale models. Pioneering work like BitNet b1.58 [MWM + 24] demonstrated that 1.58-bit LLMs can match full-precision performance while drastically reducing inference costs (latency, memory, throughput, energy) [WZS + 25]. However, while BitNet b1.58 quantizes weights to 1.58 bits, alleviating memory bandwidth bottlenecks, it retains 8-bit activations. This reliance on 8-bit precision prevents these models from fully leveraging the 4-bit computational capabilities of emerging hardware, shifting the efficiency bottleneck towards computation itself.\n\nAchieving lower bit-width activations is crucial for maximizing hardware utilization, particularly for efficient kernel design in batched inference scenarios. Research [WMW24, LPC + 24] highlights a key challenge: the non-uniform distribution of activations within LLMs. While inputs to attention and feed-forward network (FFN) layers often exhibit Gaussian-like distributions amenable to quantization, their intermediate states (outputs before final projection) contain significant outliers, hindering aggressive low-bit quantization. BitNet a4.8 [WMW24] attempted to address this by selectively using 4-bit quantization for inputs and 8-bit sparsification for intermediate state",
            "r instance, a lesion might appear in only a small part of the image, where max pooling is particularly effective. On the other hand, if the lesion is spread across a larger portion of the image, average pooling can be more beneficial. By incorporating both max pooling and average pooling in Modified-DEMNET, the model leverages each technique's strengths to effectively capture localized and global features. This approach enhances the model's ability to generalize across diverse image data, reducing the likelihood of learning patterns that are specific to the training set and mitigating the risk of overfitting.\n\nMoreover, we simplified the fully connected layer of DEMNET into two layers: a parameter-free flattened layer and a Dense layer with 100 neurons, using ReLU activation function to extract the 100 activation values. Similar to IR-BRAINNET, and considering the four classes in the Kaggle Alzheimer's dataset, a Dense layer with four neurons that uses the Softmax activation function was employed as the prediction layer. In ModifiedDEMNET, we kept all the batch normalization layers intact, as they help prevent overfitting and address issues like vanishing and exploding gradients. By stabilizing the input distributions to each layer-distributions that might otherwise fluctuate during training-batch normalization ensures more uniform activations across the network.  This  consistency  helps  address  the  issues  of  vanishing  and  exploding  gradients  by  keeping  the activa",
            "inlong Pang, Yue Cao, and Haisheng Tan. Online scheduling unbiased distributed learning over wireless edge networks. In 2021 IEEE 27th International Conference on Parallel and Distributed Systems (ICPADS) , pages 599-606. IEEE, 2021.\n- Jingoo Han, Ahmad Faraz Khan, Syed Zawad, Ali Anwar, Nathalie Baracaldo Angel, Yi Zhou, Feng Yan, and Ali R Butt. Tokenized incentive for federated learning. In Proceedings of the Federated Learning Workshop at the Association for the Advancement of Artificial Intelligence (AAAI) Conference , 2022.\n- Cengis Hasan. Incentive mechanism design for federated learning: Hedonic game approach. arXiv preprint arXiv:2101.09673 , 2021.\n- Yifan Hua, Jinlong Pang, Xiaoxue Zhang, Yi Liu, Xiaofeng Shi, Bao Wang, Yang Liu, and Chen Qian. Towards practical overlay networks for decentralized federated learning. In 2024 IEEE 32nd International Conference on Network Protocols (ICNP) , pages 1-11. IEEE, 2024.\n- Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning , pages 448-456. pmlr, 2015.\n- Jiawen Kang, Zehui Xiong, Dusit Niyato, Han Yu, YingChang Liang, and Dong In Kim. Incentive design for efficient federated learning in mobile networks: A contract theory approach. In 2019 IEEE VTS Asia Pacific Wireless Communications Symposium (APWCS) , pages 1-5. IEEE, 2019.\n- Sai Praneeth Karimireddy, Satyen Kale, Mehryar Mohri, Sashank Reddi, Sebast",
            "ken-level batch size remains large even with a batch size of 1. Large batch size typically causes gradient over-smoothing during backpropagation (JastrzÀõ ebski et al., 2018; Keskar et al., 2017; Gao and Zhong, 2020), thereby leading to a generalization gap. Specifically, the model updates with the average gradient of the tokens in the batch. As the batch size increases gradually-consider an extreme case where a single batch encompasses the entire training dataset-the gradients across batches become more similar, causing the model loss to decrease rapidly along the similar gradients and converge to a sharp minimum. More critically, in CoT distillation, the core reasoning tokens (such as the yellow and green ball in the rationale of Figure 1) constitute a small proportion of rationales, while the prevalence of similar non-reasoning tokens (e.g., those used for transition and summarization) across different rationales exacerbates gradient over-smoothing, causing the SLM to converge rapidly towards learning the expressive patterns of the LLM rather than core reasoning logic.\n\n2) Time-consuming. The SLM trained with these methods requires completing the full rationale to produce the final answer during testing, resulting in a significantly slower response time.\n\nTo address the first problem, some naive approaches, such as weighting the loss of core reasoning tokens or prompt LLMs to remove redundant expressions in rationale, do not perform well (see Appendix C). In this work, we p",
            "aud, Arthur Mensch, Elena Buchatskaya, Trevor Cai, Eliza Rutherford, Diego de Las Casas, Lisa Anne Hendricks, Johannes Welbl, Aidan Clark, et al. Training compute-optimal large language models. arXiv preprint arXiv:2203.15556 , 2022.\n- Chan-Jan Hsu, Davide Buffelli, Jamie McGowan, Feng-Ting Liao, Yi-Chang Chen, Sattar Vakili, and Da-shan Shiu. Group think: Multiple concurrent reasoning agents collaborating at token level granularity. arXiv preprint arXiv:2505.11107 , 2025.\n- Sergey Ioffe and Christian Szegedy. Batch normalization: Accelerating deep network training by reducing internal covariate shift. In International conference on machine learning , pages 448-456. pmlr, 2015.\n- Shomik Jain, Jack Lanchantin, Maximilian Nickel, Karen Ullrich, Ashia Wilson, and Jamelle Watson-Daniels. Llm output homogenization is task dependent. arXiv preprint arXiv:2509.21267 , 2025.\n- Tian Jin, Ellie Y Cheng, Zack Ankner, Nikunj Saunshi, Blake M Elias, Amir Yazdanbakhsh, Jonathan Ragan-Kelley, Suvinay Subramanian, and Michael Carbin. Learning to keep a promise: Scaling language model decoding parallelism with learned asynchronous decoding. arXiv preprint arXiv:2502.11517 , 2025.\n- Jared Kaplan, Sam McCandlish, Tom Henighan, Tom B Brown, Benjamin Chess, Rewon Child, Scott Gray, Alec Radford, Jeffrey Wu, and Dario Amodei. Scaling laws for neural language models. arXiv preprint arXiv:2001.08361 , 2020.\n- Yifei Li, Zeqi Lin, Shizhuo Zhang, Qiang Fu, Bei Chen, Jian-Guang Lou, and Weizhu Chen. Mak"
        ]
    },
    {
        "answer": "Transformer models handle an entire sequence at once by replacing recurrence with **self‚Äëattention**.  \nIn a self‚Äëattention layer each token‚Äôs query, key and value vectors are computed and then compared with the queries, keys and values of **all other tokens** in the same step (e.g., the dot‚Äëproduct attention‚ÄØ\\( \\text{softmax}(QK^{T}/\\sqrt{d_k})V \\)). Because this operation is performed on the whole token matrix simultaneously, the model can weigh the importance of every token relative to every other token in parallel, rather than stepping through the sequence one element at a time as RNNs do.  \n\nTo keep track of order while still processing in parallel, **positional encodings** are added to the token embeddings before the attention computation. This lets the model learn contextualized representations for the full sequence efficiently on parallel hardware.  \n\n*Source: the description of the Transformer‚Äôs parallel processing and self‚Äëattention mechanisms (‚Äúparallel, rather than sequentially‚Ä¶ By eschewing recurrence‚Ä¶ the Transformer has achieved‚Ä¶ greater parallelisation‚Ä¶ its powerful self‚Äëattention mechanism‚Ä¶‚Äù)‚Äã.*",
        "retrieval_context": [
            "parallel, rather than sequentially as in traditional recurrent  neural  networks  (RNNs)  or  convolutional  neural networks (CNNs). By eschewing recurrence and convolutions  in  favour  of  attention-only  mechanisms,  the Transformer  has  achieved  unprecedented  efficiency  and scalability in sequence modelling, enabling greater parallelisation and accelerating training on large datasets. The immediate impact of the Transformer architecture was most pronounced in Natural Language Processing (NLP), demonstrated  by  substantial  improvements  in  translation tasks,  language  modelling  and  contextual  understanding. However,  its  conceptual  innovation  rapidly  transcended NLP, catalysing advancements across diverse domains including computer  vision  [8],  protein folding [9]  and genomic sequence analysis [5].\n\nIn the context of gene enhancer prediction, the Transformer's capability to capture intricate long-range dependencies and sequence-contextual interaction, without losing efficiency or interpretability, positions it uniquely as an ideal architecture. It's  p owerful self-attention  mechanism facilitates  enhanced pattern  recognition  within  DNA  sequences,  enabling  the sophisticated  modelling  of  regulatory  elements  such  as enhancers,  which  typically  involve  complex  interactions distributed  across  extensive  genomic  regions.  Inspired  by breakthroughs utilising the Transformer in NLP, researchers developed genomic language models (gLMs) -large",
            "rd or a masked word) and language models are trained to predict them, is known as self-supervised learning. Transformer-based models, with their parallel processing capabilities and ability to capture correlations across entire sequences, have achieved state-of-the-art (SOTA) performance [91, 92]. A more advanced training diagram is the text-to-text framework. This kind of training diagram unified multiple kinds of tasks, including translation, question answering, classification, formulated and feeding to model as input and training it as a generative model to predict target text. This framework, which is named 'T5' benefits using the same model, loss function, hyperparameters, etc. across a diverse set of tasks [93].\n\nPre-training In many supervised learning problems, input data is represented by multiple features, comprising numerical or categorical information that can aid in making predictions. Scratch-trained models, which initialize and train all parameters from the ground up using task-specific datasets, typically require numerous iterations to converge fully on a single task. In general, transformer-based language models fall into two categories: scratch-trained models and pre-trained models. LLMs apply transformer-based pre-trained models that are trained from large amounts of unlabeled data and then fine-tuned for specific tasks. Pre-training learns general information from unlabeled data which can improve the convergence rate of the target tasks and often has bette",
            "**Input**: The diagram starts with two input signals, labeled as \"WB Signal\" and \"NB Signal.\" These signals are likely to be audio signals, given the context of the diagram.\n2. **Encoder**: The input signals are fed into an encoder, which is labeled as \"Encoder x12.\" The encoder is a crucial component of the transformer model, responsible for transforming the input signals into a format that can be processed by the decoder.\n3. **Self-Attention**: Within the encoder, there is a self-attention mechanism. This mechanism allows the model to weigh the importance of different parts of the input signal relative to each other. This is a key feature of transformer models, enabling them to handle long-\n\n## 3.1.2. Evaluation datasets\n\nTo evaluate the general performance of ASR and ST, we use two in-house test sets along with the publicly available Fleurs [21] dataset. Our in-house test sets consist of 1,000 samples of male and female speech from daily conversations, with reference translations curated by professional translators. Additionally, to assess the model under different bandwidth conditions, we generate NB/WB variants of the test sets using the same conversion methods as in training.\n\n## 3.2. Model\n\nOur base model follows a Transformer-based sequence-tosequence architecture [22], consisting of a 12-layer encoder and a 6-layer decoder. We employ sinusoidal positional embeddings and a pre-normalization Transformer structure with GLU activations and 8 attention heads. The FFN inne",
            "ion across the entire sequence. The \"multi-head\" aspect refers to multiple attention heads operating simultaneously to capture diverse contextual features. Inside a single attention head, a token output embedding in a sequence is computed and fused with other tokens in the context with a proper causal mask. Such global level attention mechanic enables efficient information fusion along available context windows.\n\nSelf-supervised training methods Language models are trained using self-supervised learning methods [85]. Unlike supervised learning, which typically requires human annotations, language models can leverage vast amounts of unannotated text data [86]. The objective of unsupervised learning is to analyze unlabeled data by identifying and capturing its meaningful properties. Neural\n\nnetworks can extend some of these approaches. For example, autoencoders compress data into a lowdimensional representation through a hidden layer known as the bottleneck layer and then reconstruct the original input data from this representation [87, 88, 89, 90]. Language models leverage either the next word in a sentence as a natural label for the context or artificially mask a known word and predict it. This method, where unstructured data generates its own labels (e.g., predicting the next word or a masked word) and language models are trained to predict them, is known as self-supervised learning. Transformer-based models, with their parallel processing capabilities and ability to capture",
            "ooped transformers to achieve scalable test-time computation with negligible latency overhead.\n- We introduce Cross-Loop Parallelism (CLP) and an Efficient Representation Enhancement technique (KV-cache sharing with gated SWA) to overcome the critical latency and memory bottlenecks of traditional looped models.\n- We demonstrate through extensive experiments on both in-house and open-source models that PLT significantly outperforms vanilla transformer baselines in accuracy while adding minimal latency.\n- We show that PLT is far more efficient than vanilla looped transformers, and can even enable a shallower, more efficient PLT model (e.g., 1.7B activated parameters) to achieve superior performance and lower latency than a much larger vanilla model (e.g., 2.5B activated parameters).\n\n## 2 Method\n\nFigure 2 Training and inference pipeline of PLT with loop count L =3 . Training (Left): Same Colored boxes trace how input tokens traverse the loops to predict their targets (e.g., token T 1 passes three loops to predict T 4 , consistent with Figure 1b). Training is parallel along the token dimension and serial along the loop dimension. Inference (Right): Parallelized forward pass of PLT when decoding T 4 and T 5 in a Loop Transformer with L =3 . Because there are no horizontal (same-step, cross-loop) activation dependencies during training, computations within the same step (each row; see the blue dashed box) run in parallel during decoding.\n\nThe image depicts a diagram of a Loop Tran",
            "on enables parallel processing of the entire sequence, making the transformers computationally efficient. A set of several attention layers running in parallel is called MultiHead Attention.\n\nThe Transformer architecture, shown in Fig. 1 (a), comprises Encoder and Decoder sections, similar to many CNN-based generators. However, the encoder is now a stack of identical layers, concatenating a multi-head self-attention mechanism and a fully connected feed-forward network. The decoder is also a stack of identical layers, in which each layer has an additional sub-layer to perform multi-head attention over the output of the encoder stack.\n\nMathematically, the attention function is computed from inputs: query Q , keys K , and values V . The matrix of outputs of attention function is\n\n$$A t t e n t i o n ( Q , K , V ) = \\text {softmax} ( \\frac { Q K ^ { T } } { \\sqrt { d _ { k } } } ) V ,$$\n\nwhere d k is a dimension of K . The term QK T is Dot-Product Attention, which yields a high similarity value when the two words are closely related. If Q and K are from the same sentence, Eq. 1 refers to self-attention, but if Q and K are from different sentences, it is referred to as cross-attention. Within the network, multi-head attention is actually employed to concurrently process attention and enable the model to collectively focus on information from distinct representation subspaces at various positions through the learnable parameters W s.\n\n$$M u t i { \\text {Head} } ( Q , K , V ) & = \\t",
            "olving multiple loops. The diagram is structured in a hierarchical manner, with each level of the process represented by a series of boxes connected by arrows. Here is a detailed description of the image:\n\n1. **Top Level**:\n   - The diagram starts with a single box labeled \\( t_4 \\).\n   - This box is connected to a dashed box labeled \\( \\text{loop-1} \\).\n   - The dashed box \\( \\text{loop-1} \\) is connected to another box labeled \\( t_3 \\).\n   - The connection between \\( t_3 \\) and \\( \\text{loop-1} \\) is a solid line.\n\n2. **Second Level**:\n   - The box \\( t_3 \\) is connected to a dashed box labeled \\( \\text{loop-2} \\).\n   - The dashed box \\( \\text{loop-2\n\nFigure 1 Illustration of the computation flow. (a) Vanilla loop transformer, where each loop in each token should be computed in serial manner. (b) Parallel loop transformer (PLT), where transformer loops within the same blue dashed box can be computed in parallel.\n\napplications, effectively negating their primary advantages.\n\nTo resolve this tension between effective depth and inference speed, we introduce the Parallel Loop Transformer (PLT) . PLT is a novel architecture designed to unlock the performance benefits of deep, looped computation while maintaining the approximate inference latency of a standard, non-looped transformer. As shown in Figure 1b, the core principle of PLT is to break the sequential loop dependency by parallelizing the computation of different loops across different tokens.\n\nOur approach consists of tw",
            "## Parallel Loop Transformer for Efficient Test-Time Computation Scaling\n\n## ByteDance Seed\n\nFull author list in Contributions\n\n## Abstract\n\nLarge Language Models (LLMs) are powerful but often too slow and costly for real-world use during inference. Looped transformers save on parameters by reusing the same weights for multiple computational steps, or \"loops.\" However, this approach has a major flaw: the loops run one after another, causing inference latency and memory requirements to increase with each added loop. This makes them impractical for fast applications. To solve this problem, we introduce the Parallel Loop Transformer (PLT). PLT is a new architecture that delivers the performance benefits of a deep, looped model but with the low latency of a standard, non-looped model. PLT works using two key techniques. First, Cross-Loop Parallelism (CLP) breaks the sequential dependency by computing different loops for different tokens at the same time, all within a single pass. Second, to prevent memory costs from growing, we use an Efficient Representation Enhancement strategy. This method shares the memory (KV cache) from the first loop with all other loops. It then uses a Gated Sliding-Window Attention (G-SWA) to combine this shared global information with local information, maintaining high accuracy. Our experiments show that PLT achieves the high accuracy of a traditional looped model but with almost no extra latency or memory cost compared to a standard transformer.\n\nDate",
            "ers offer an alternative design, introducing task-specific modules that operate concurrently with the primary layers of the pre-trained model [217]. Unlike serial adapters [244], which modify intermediate representations directly, parallel adapters [217] process task-specific representations alongside the model's primary computations, reducing interference while maintaining independent pathways for task-specific learning. Examples include Uni-Adapter [421], UniPT [137], AdaptFormer [81], ConvPass [73], and PEMT [389]. For instance, AdaptFormer [81] embeds parallel adapters [217] within transformer-based architectures to improve adaptability in multi-task contexts, while ConvPass [73] uses convolutional modules for enhanced performance in vision-oriented tasks. The operation of a parallel adapter [217] can be expressed mathematically as:\n\n$$h _ { p a r a l l } = W _ { p a r a l l } h _ { i n } + b _ { p a r a l l }$$\n\nwhere W parallel and b parallel represent learnable parameters. The final output combines the primary model's representation h main with the adapter's output:\n\n$$h _ { o u t } = h _ { \\text {main} } + \\alpha h _ { \\text {parallel} }$$\n\nwith Œ± is a scaling factor that adjusts the influence of the adapter's contribution. These designs, categorized under Additive Tuning, are illustrated in Figure 3, highlighting the parallel integration of UniAdapters across visual, textual, and cross-modal pathways.\n\n## 5.2 Hybrid adapters\n\nHybrid adapters synthesize the benefits o"
        ]
    },
    {
        "answer": "The documents describe several concrete advantages of adding residual connections to neural‚Äënetwork architectures:\n\n* **Mitigates vanishing‚Äëgradient problems** ‚Äì By providing shortcut paths that bypass one or more layers, residual connections allow gradients to flow more directly backward through the network, preventing the exponential decay that can occur in deep recurrent or convolutional models (see the ConvLSTM discussion).  \n* **Improves feature retention** ‚Äì The identity mapping \\(H_t = F(X_t, H_{t-1}) + H_{t-1}\\) keeps the original hidden state available at each step, so key spatial‚Äëtemporal information is preserved even after many transformations.  \n* **Enables stable training of very deep models** ‚Äì Because the gradient can propagate through the shortcut, deeper stacks of layers become trainable without the loss of signal that normally limits depth.  \n* **Accelerates convergence** ‚Äì Experiments reported in the text show that networks with residual connections converge faster and achieve lower mean‚Äësquared error (MSE), especially under challenging conditions such as low‚Äëvoltage lithium‚Äëdendrite growth simulations.  \n* **Enhances overall performance** ‚Äì The residual ConvLSTM model ‚Äúsignificantly reduces MSE‚Äù and better captures long‚Äërange dependencies, indicating that residual pathways lead to more accurate predictions and better generalization.\n\nThese points collectively illustrate why residual connections are widely used: they provide shortcut pathways that preserve information and gradients, facilitate deeper architectures, speed up learning, and often improve final model accuracy.",
        "retrieval_context": [
            "losely matches the\n\nsimulation output in terms of interface location and morphological consistency, suggesting strong generalization to physically plausible dynamics.\n\n## Discussion\n\nThe integration of residual connections within the ConvLSTM framework provides a significant advantage by addressing the vanishing gradient problem and enhancing feature retention across layers. In conventional ConvLSTM architectures, gradient propagation can deteriorate due to recursive multiplications during backpropagation through time (BPTT), leading to poor long-range dependency modeling. By incorporating residual connections, the network effectively bypasses intermediate transformations, allowing a more stable gradient flow and accelerating convergence.\n\nFrom a mathematical perspective, residual connections reformulate the recurrent update in ConvLSTM as:\n\n$$H _ { t } = F ( X _ { t } , H _ { t - 1 } ) + H _ { t - 1 } ,$$\n\nwhere F ( X t , H t -1 ) represents the transformation applied by the ConvLSTM cell. This identity mapping ensures that the state propagation retains key spatial-temporal features without excessive transformation decay. Consequently, the network can better capture lithium dendrite growth patterns over extended time horizons, particularly under conditions where gradual morphological changes occur.\n\nThe experimental results further validate this hypothesis, demonstrating that the residual ConvLSTM significantly reduces MSE, particularly at lower voltages. One plausible expla",
            "e relevance for a given construct, while higher values indicate better predictive power.\n\nDisclaimer/Publisher's  Note: The  statements,  opinions  and  data  contained  in  all  publications  are  solely  those  of  the  individual author(s) and contributor(s) and not of the publisher and/or the editor(s). This publisher and/or the editor(s) disclaim responsibility for any injury to people or property resulting from any ideas, methods, instructions or products referred to in the content.\n\n_________________________________________________________________________________\n\n¬© Copyright (2025): Author(s). The licensee is the journal publisher. This is an Open Access article distributed under the terms of  the  Creative  Commons  Attribution  License  (http://creativecommons.org/licenses/by/4.0),  which  permits  unrestricted  use, distribution, and reproduction in any medium, provided the original work is properly cited.\n\nPeer-review history: The peer review history for this paper can be accessed here: https://pr.sdiarticle5.com/review-history/135041",
            " [5] Jingzhou Chen, Peng Wang, Jian Liu, and Yuntao Qian. Label relation graphs enhanced hierarchical residual network for hierarchical multi-granularity classification. In IEEE Conference on Computer Vision and Pattern Recognition , pages 4858-4867, 2022.\n- [6] Ekin D Cubuk, Barret Zoph, Jonathon Shlens, and Quoc V Le. Randaugment: Practical automated data augmentation with a reduced search space. In Annual Conference on Neural Information Processing Systems , pages 702-703, 2020.\n- [7] Yin Cui, Menglin Jia, Tsung-Yi Lin, Yang Song, and Serge Belongie. Class-balanced loss based on effective number of samples. In IEEE Conference on Computer Vision and Pattern Recognition , pages 9268-9277, 2019.\n- [8] Yingxiao Du and Jianxin Wu. No one left behind: Improving the worst categories in long-tailed learning. In IEEE Conference on Computer Vision and Pattern Recognition , pages 15804-15813, 2023.\n- [9] Jie Feng, Yong Li, Chao Zhang, Funing Sun, Fanchao Meng, Ang Guo, and Depeng Jin. Deepmove: Predicting human mobility with attentional recurrent networks. In International World Wide Web Conferences , pages 1459-1468, 2018.\n- [10] Shanshan Feng, Feiyu Meng, Lisi Chen, Shuo Shang, and Yew Soon Ong. Rotan: A rotation-based temporal attention network for time-specific next poi recommendation. In ACM Knowledge Discovery and Data Mining , pages 759-770, 2024.\n- [11] Letian Gong, Yan Lin, Yiwen Lu, Xuedi Han, Yichen Liu, Shengnan Guo, Youfang Lin, Huaiyu Wan, et al. Mobility-llm: Learning ",
            "d Seyed Mohammad Ghodsi. 'Benefits of collaborative learning'. In: Procedia-social and behavioral sciences 31 (2012), pp. 486-490. DOI: 10.1016/ j.sbspro.2011.12.091 .\n- [274] Erik R¬¥ ua et al. 'Automatic detection to inventory road slopes using open LiDAR point clouds'. In: International Journal of Applied Earth Observation and Geoinformation 118 (Apr. 2023), p. 103225. ISSN: 15698432. DOI: 10 . 1016 / j . jag . 2023 . 103225 . (Visited on 12/22/2023).\n- [275] Vijay Badrinarayanan, Alex Kendall, and Roberto Cipolla. 'Segnet: A deep convolutional encoder-decoder architecture for image segmentation'. In: IEEE transactions on pattern analysis and machine intelligence 39.12 (2017), pp. 2481-2495. DOI: 10.1109/TPAMI.2016. 2644615 .\n- [276] Charles Ruizhongtai Qi et al. 'Pointnet++: Deep hierarchical feature learning on point sets in a metric space'. In: Advances in neural information processing systems 30 (2017).\n- [277] Liang-Chieh Chen et al. 'Encoder-Decoder with Atrous Separable Convolution for Semantic Image Segmentation'. In: Proceedings of the European Conference on Computer Vision (ECCV) . Ed. by Vittorio Ferrari et al. 2018, pp. 801-818. DOI: 10.1007/978-3-030-01234-2_49 .\n- [278] Christian Szegedy et al. 'Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning'. In: Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence (AAAI) . Ed. by AAAI. Vol. 31. 1. AAAI Press, 2017. DOI: 10.1109/ICCC51575.2020. 9345042 .\n- [279] Israel ",
            "riants of MAL and MAG for our Atlas model. The architectures are illustrated in Figure 3. For models with deep memory architectures we use 2-layer MLP with residual connections:\n\n$$\\mathcal { M } ( \\cdot ) = ( \\cdot ) + W _ { 1 } \\sigma ( W _ { 2 } ( \\cdot ) ) .$$\n\nWe further extend this memory architecture, which is commonly used in recent studies (Behrouz, Razaviyayn, et al. 2025; Behrouz, Zhong, et al. 2024; Irie et al. 2021), to gated MLP layer as:\n\n$$\\mathcal { M } ( \\cdot ) = ( \\cdot ) + W _ { 1 } \\left ( \\sigma \\left ( W _ { 2 } ( \\cdot ) \\right ) \\otimes W _ { 3 } ( \\cdot ) \\right ) ,$$\n\nwhere ùëä 1 , ùëä 2 , ùëä 3 are linear learnable matrices. We refer to Atlas with the above memory architecture as Atlas++.\n\n## 6 Experiments\n\nNext, we evaluate the performance of Atlas, OmegaNet, DeepTransformers, and Dot in language modeling, commonsense reasoning, needle in haystack, and in-context recall tasks. Although we also discussed several other variants, such as SWLA, in our experiments we focus on the above models so in addition to comparison with state-of-the-art models, we also answer the following questions:\n\n1. Is deep memory effective for softmax attention? (see Table 2 - comparison of Transformer++ and DeepTransformers)\n\n2. Does the use of Omega improve the performance softmax attention? (see Table 2 - comparison of Transformer++, DeepTransformers, and Dot)\n3. Does the Omega rule provide more expressive memory update? (see Table 2 and Table 6 - the performance of OmegaNet,",
            " the initial stage where pre-trained weights are loaded into the network. The pre-trained weights are represented by a blue box with the label \"Pretrained Weights.\"\n\n2. **Residual Block**:\n   - The diagram shows a residual block, which is a fundamental component of the ResNet architecture. The block is represented by a triangular shape with a label \"r.\" Inside the block, there is a summation symbol (Œ£) indicating the summation of weights.\n\n3. **Residual Connection**:\n   - The residual connection is depicted as a line\n\ncore low-rank decomposition techniques, adaptive and dynamic rank methods, and enhanced LoRA variants for specific tasks and fine-tuning efficiency. This paper provides a detailed overview of these methods and their applications in large-scale machine learning models. Reparameterized PEFT addresses the computational and memory constraints of fine-tuning large-scale models by introducing low-rank parameterization techniques. These approaches focus on reparameterizing the delta weight matrix ( ‚àÜ W ) into a low-dimensional form, significantly reducing the number of trainable parameters. The techniques can be classified into three main categories: core low-rank decomposition , adaptive and dynamic rank methods , and enhanced LoRA variants tailored for specific tasks.\n\n## 5.6.1 Low-Rank Decomposition\n\nThe foundation of reparameterized PEFT lies in low-rank decomposition , where the parameter update matrix ‚àÜ W ‚àà R d √ó d is approximated as the product of two low-rank m",
            "ting. In Advances in Neural Information Processing Systems , volume 28, pages 802-810, 2015.\n- [2] K. He, X. Zhang, S. Ren, and J. Sun. Deep residual learning for image recognition. In Proc. IEEE Conf. Comput. Vis. Pattern Recognit. (CVPR) , pages 770-778, 2016.\n- [3] J. E. Guyer, W. J. Boettinger, J. A. Warren, and G. B. McFadden. Phase-field modeling of electrochemistry i: Equilibrium. Phys. Rev. E , 69:021603, 2004.\n- [4] K. Chen, J. S. Smith, and M. Z. Bazant. Phase-field modeling of lithium dendrite growth with open-source software. ACS Appl. Mater. Interfaces , 12(45):50058-50068, 2020.\n- [5] Z. Mu, Z. Guo, and Y.-H. Lin. Simulation of 3-d lithium dendritic evolution under multiple electrochemical states: A parallel phase field approach. Energy Storage Mater. , 24:346-354, 2020.\n- [6] Fei Wang, Mengqing Jiang, Chen Qian, Shuo Yang, Cheng Li, Honggang Zhang, Xiaogang Wang, and Xiaoou Tang. Residual attention network for image classification. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 6450-6458, 2017.\n- [7] Du Tran, Heng Wang, Lorenzo Torresani, Jamie Ray, Yann LeCun, and Manohar Paluri. A closer look at spatiotemporal convolutions for action recognition. In Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition , pages 6450-6459, 2018.\n- [8] Ashish Vaswani, Noam Shazeer, Niki Parmar, Jakob Uszkoreit, Llion Jones, Aidan N. Gomez, Lukasz Kaiser, and Illia Polosukhin. Attention is all you need. In Advances",
            "Lee, M. Ducoffe, Potential, challenges and future directions for deep learning in prognostics and health management applications, Engineering Applications of Artificial Intelligence 92 103678.\n- [3] C. Gao, Z. Wang, Y. Guo, H. Wang, H. Yi, MPINet: Multiscale Physics-Informed Network for Bearing Fault Diagnosis With Small Samples, IEEE Transactions on Industrial Informatics 20 (12) (2024) 14371-14380. doi:10.1109/TII.2024.3452174 .\n- [4] A. Diedrich, O. Niggemann, On Residual-based Diagnosis of Physical Systems, Engineering Applications of Artificial Intelligence 109 (2022) 104636. doi:https://doi.org/10.1016/j.engappai.2021.104636 .\n- [5] R. Reiter, A Theory of Diagnosis from First Principles, Artificial intelligence 32 (1) (1987) 57-95.\n- [6] I. Pill, T. Quaritsch, RC-Tree: A variant avoiding all the redundancy in Reiter's minimal hitting set algorithm, in: 2015 IEEE International Symposium on Software Reliability Engineering Workshops (ISSREW), 2015, pp. 78-84. doi:10.1109/ISSREW.2015.7392050 .\n- [7] E. Mu≈°kardin, I. Pill, F. Wotawa, CatIO - A Framework for Model-Based Diagnosis of Cyber-Physical Systems, in: Foundations of Intelligent Systems: 25th Int. Symp., ISMIS 2020 , Graz, Austria, September 23-25, 2020, Proceedings, 2020, p. 267-276. doi:10.1007/978-3-030-59491-6\\_25 .\n- [8] A. Mohammadi, M. Krysander, D. Jung, Analysis of grey-box neural network-based residuals for consistencybased fault diagnosis, IFAC-PapersOnLine 55 (6) (2022) 1-6.\n- [9] A. Slimani, P. Ribot, E.",
            "ssing systems , 31, 2018.\n- Loic Simon, Ryan Webster, and Julien Rabin. Revisiting precision recall definition for generative modeling. In International Conference on Machine Learning , pages 57995808. PMLR, 2019.\n- K. Simonyan and A. Zisserman. Very deep convolutional networks for large-scale image recognition. URL https://ora.ox.ac.uk/objects/uuid: 60713f18-a6d1-4d97-8f45-b60ad8aebbce .\n- Rodrigue Siry, Ryan Webster, Loic Simon, and Julien Rabin. On the theoretical equivalence of several trade-off curves assessing statistical proximity. Journal of Machine Learning Research , 24(185):1-34, 2023. URL http://jmlr.org/papers/v24/21-0607.html .\n- Christian Szegedy, Vincent Vanhoucke, Sergey Ioffe, Jon Shlens, and Zbigniew Wojna. Rethinking the Inception Architecture for Computer Vision. In 2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR) , pages 2818-2826. IEEE. ISBN 978-1-4673-8851-1. doi: 10.1109/CVPR.2016.308. URL http://ieeexplore.ieee.org/ document/7780677/ .\n- Roman Vershynin. High-dimensional probability: An introduction with applications in data science , volume 47. Cambridge university press, 2018."
        ]
    }
]